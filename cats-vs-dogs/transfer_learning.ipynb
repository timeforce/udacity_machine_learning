{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import required packages\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import load_model, Model\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.applications import ResNet50, VGG16, InceptionV3\n",
    "from keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "# from utils import make_parallel\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from keras.utils import to_categorical\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Apply pre-trained model on training data directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here I use VGG16 model.**\n",
    "\n",
    "**Steps** \n",
    "\n",
    "- Load image：cv2.imread\n",
    "- Modify size：cv2.resize, the input image size of VGG16 is 224 * 224\n",
    "- Modify dimension：np.expand_dims, the input of VGG16 is 3 channels\n",
    "- Preprocess：preprocess_input，VGG16 uses mean substraction\n",
    "- Predict：model.predict\n",
    "- Decode result：decode_predictions, decode the results into a list of tuples (class, description, probability). E.g. (u'n02504013', u'Indian_elephant', 0.82658225)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 18145280/553467096 [..............................] - ETA: 5636 - ETA: 2850 - ETA: 1978 - ETA: 1497 - ETA: 1407 - ETA: 1146 - ETA: 8569s - ETA: 6887 - ETA: 6762 - ETA: 5292 - ETA: 5226 - ETA: 5276 - ETA: 5209 - ETA: 4976 - ETA: 4942 - ETA: 4689 - ETA: 4675 - ETA: 4399 - ETA: 4375 - ETA: 4226 - ETA: 4208 - ETA: 4021 - ETA: 4012 - ETA: 3902 - ETA: 3894 - ETA: 3877 - ETA: 3755 - ETA: 3682 - ETA: 3658 - ETA: 3648 - ETA: 3826 - ETA: 3605 - ETA: 3611 - ETA: 3594 - ETA: 3608 - ETA: 3718 - ETA: 3622 - ETA: 3633 - ETA: 3678 - ETA: 3703 - ETA: 3735 - ETA: 3937 - ETA: 3963 - ETA: 3893 - ETA: 3925 - ETA: 3917 - ETA: 4047 - ETA: 4052 - ETA: 4120 - ETA: 4170 - ETA: 4207 - ETA: 4274 - ETA: 4312 - ETA: 4366 - ETA: 4422 - ETA: 4456 - ETA: 4438 - ETA: 4505 - ETA: 4551 - ETA: 4607 - ETA: 4622 - ETA: 4605 - ETA: 4646 - ETA: 4704 - ETA: 4738 - ETA: 4717 - ETA: 4765 - ETA: 4797 - ETA: 4758 - ETA: 4833 - ETA: 4849 - ETA: 4919 - ETA: 4966 - ETA: 4975 - ETA: 5011 - ETA: 4999 - ETA: 5006 - ETA: 4974 - ETA: 4956 - ETA: 4892 - ETA: 4864 - ETA: 4855 - ETA: 4733 - ETA: 4754 - ETA: 4763 - ETA: 4770 - ETA: 4744 - ETA: 4704 - ETA: 4654 - ETA: 4595 - ETA: 4575 - ETA: 4521 - ETA: 4481 - ETA: 4427 - ETA: 4392 - ETA: 4426 - ETA: 4327 - ETA: 4289 - ETA: 4280 - ETA: 4266 - ETA: 4242 - ETA: 4216 - ETA: 4196 - ETA: 4186 - ETA: 4176 - ETA: 4187 - ETA: 4126 - ETA: 4108 - ETA: 4118 - ETA: 4113 - ETA: 4143 - ETA: 4103 - ETA: 4112 - ETA: 4120 - ETA: 4155 - ETA: 4162 - ETA: 4139 - ETA: 4154 - ETA: 4171 - ETA: 4171 - ETA: 4182 - ETA: 4228 - ETA: 4238 - ETA: 4229 - ETA: 4216 - ETA: 4234 - ETA: 4283 - ETA: 4285 - ETA: 4312 - ETA: 4309 - ETA: 4331 - ETA: 4346 - ETA: 4366 - ETA: 4420 - ETA: 4440 - ETA: 4466 - ETA: 4493 - ETA: 4540 - ETA: 4602 - ETA: 4722 - ETA: 4949 - ETA: 5099 - ETA: 5189 - ETA: 5253 - ETA: 5307 - ETA: 5421 - ETA: 5543 - ETA: 5585 - ETA: 5672 - ETA: 5722 - ETA: 5773 - ETA: 5822 - ETA: 5859 - ETA: 5889 - ETA: 5923 - ETA: 5943 - ETA: 5958 - ETA: 5992 - ETA: 6019 - ETA: 6033 - ETA: 6046 - ETA: 6056 - ETA: 6072 - ETA: 6059 - ETA: 6060 - ETA: 6054 - ETA: 6048 - ETA: 6031 - ETA: 6021 - ETA: 6049 - ETA: 6031 - ETA: 5984 - ETA: 5981 - ETA: 5958 - ETA: 5952 - ETA: 5936 - ETA: 5927 - ETA: 5921 - ETA: 5891 - ETA: 5869 - ETA: 5854 - ETA: 5838 - ETA: 5836 - ETA: 5816 - ETA: 5790 - ETA: 5770 - ETA: 5750 - ETA: 5729 - ETA: 5725 - ETA: 5712 - ETA: 5707 - ETA: 5694 - ETA: 5688 - ETA: 5677 - ETA: 5659 - ETA: 5643 - ETA: 5617 - ETA: 5602 - ETA: 5579 - ETA: 5562 - ETA: 5546 - ETA: 5511 - ETA: 5511 - ETA: 5521 - ETA: 5473 - ETA: 5471 - ETA: 5470 - ETA: 5467 - ETA: 5446 - ETA: 5445 - ETA: 5458 - ETA: 5442 - ETA: 5433 - ETA: 5433 - ETA: 5432 - ETA: 5430 - ETA: 5421 - ETA: 5408 - ETA: 5411 - ETA: 5408 - ETA: 5409 - ETA: 5407 - ETA: 5401 - ETA: 5400 - ETA: 5399 - ETA: 5395 - ETA: 5390 - ETA: 5378 - ETA: 5400 - ETA: 5387 - ETA: 5379 - ETA: 5393 - ETA: 5389 - ETA: 5388 - ETA: 5397 - ETA: 5392 - ETA: 5402 - ETA: 5401 - ETA: 5412 - ETA: 5412 - ETA: 5420 - ETA: 5412 - ETA: 5410 - ETA: 5414 - ETA: 5435 - ETA: 5432 - ETA: 5443 - ETA: 5458 - ETA: 5458 - ETA: 5490 - ETA: 5488 - ETA: 5507 - ETA: 5518 - ETA: 5548 - ETA: 5564 - ETA: 5575 - ETA: 5585 - ETA: 5613 - ETA: 5622 - ETA: 5632 - ETA: 5644 - ETA: 5648 - ETA: 5657 - ETA: 5667 - ETA: 5676 - ETA: 5685 - ETA: 5694 - ETA: 5703 - ETA: 5713 - ETA: 5716 - ETA: 5725 - ETA: 5733 - ETA: 5742 - ETA: 5763 - ETA: 5758 - ETA: 5788 - ETA: 5796 - ETA: 5811 - ETA: 5821 - ETA: 5836 - ETA: 5845 - ETA: 5866 - ETA: 5873 - ETA: 5883 - ETA: 5892 - ETA: 5897 - ETA: 5906 - ETA: 5915 - ETA: 5929 - ETA: 5937 - ETA: 5945 - ETA: 5963 - ETA: 5961 - ETA: 5964 - ETA: 5973 - ETA: 5981 - ETA: 5987 - ETA: 5990 - ETA: 5993 - ETA: 6002 - ETA: 6011 - ETA: 6012 - ETA: 6019 - ETA: 6019 - ETA: 6028 - ETA: 6036 - ETA: 6037 - ETA: 6040 - ETA: 6048 - ETA: 6042 - ETA: 6045 - ETA: 6047 - ETA: 6054 - ETA: 6062 - ETA: 6063 - ETA: 6070 - ETA: 6067 - ETA: 6061 - ETA: 6070 - ETA: 6077 - ETA: 6074 - ETA: 6074 - ETA: 6081 - ETA: 6088 - ETA: 6088 - ETA: 6080 - ETA: 6086 - ETA: 6086 - ETA: 6084 - ETA: 6106 - ETA: 6094 - ETA: 6114 - ETA: 6110 - ETA: 6136 - ETA: 6143 - ETA: 6149 - ETA: 6156 - ETA: 6163 - ETA: 6165 - ETA: 6172 - ETA: 6173 - ETA: 6178 - ETA: 6185 - ETA: 6184 - ETA: 6190 - ETA: 6197 - ETA: 6198 - ETA: 6204 - ETA: 6211 - ETA: 6214 - ETA: 6207 - ETA: 6215 - ETA: 6223 - ETA: 6228 - ETA: 6228 - ETA: 6237 - ETA: 6233 - ETA: 6239 - ETA: 6245 - ETA: 6249 - ETA: 6242 - ETA: 6245 - ETA: 6249 - ETA: 6256 - ETA: 6262 - ETA: 6268 - ETA: 6266 - ETA: 6271 - ETA: 6289 - ETA: 6291 - ETA: 6301 - ETA: 6308 - ETA: 6321 - ETA: 6326 - ETA: 6332 - ETA: 6332 - ETA: 6344 - ETA: 6351 - ETA: 6357 - ETA: 6363 - ETA: 6374 - ETA: 6381 - ETA: 6387 - ETA: 6392 - ETA: 6387 - ETA: 6396 - ETA: 6417 - ETA: 6423 - ETA: 6429 - ETA: 6442 - ETA: 6432 - ETA: 6438 - ETA: 6444 - ETA: 6450 - ETA: 6456 - ETA: 6461 - ETA: 6467 - ETA: 6473 - ETA: 6465 - ETA: 6470 - ETA: 6476 - ETA: 6480 - ETA: 6486 - ETA: 6487 - ETA: 6483 - ETA: 6493 - ETA: 6486 - ETA: 6493 - ETA: 6499 - ETA: 6505 - ETA: 6510 - ETA: 6515 - ETA: 6522 - ETA: 6521 - ETA: 6522 - ETA: 6527 - ETA: 6532 - ETA: 6527 - ETA: 6550 - ETA: 6548 - ETA: 6543 - ETA: 6544 - ETA: 6550 - ETA: 6556 - ETA: 6562 - ETA: 6568 - ETA: 6572 - ETA: 6578 - ETA: 6592 - ETA: 6582 - ETA: 6598 - ETA: 6603 - ETA: 6613 - ETA: 6620 - ETA: 6625 - ETA: 6630 - ETA: 6645 - ETA: 6649 - ETA: 6650 - ETA: 6656 - ETA: 6665 - ETA: 6670 - ETA: 6676 - ETA: 6682 - ETA: 6687 - ETA: 6679 - ETA: 6688 - ETA: 6692 - ETA: 6697 - ETA: 6701 - ETA: 6707 - ETA: 6712 - ETA: 6717 - ETA: 6721 - ETA: 6721 - ETA: 6726 - ETA: 6730 - ETA: 6734 - ETA: 6750 - ETA: 6745 - ETA: 6749 - ETA: 6744 - ETA: 6755 - ETA: 6759 - ETA: 6758 - ETA: 6780 - ETA: 6784 - ETA: 6778 - ETA: 6789 - ETA: 6794 - ETA: 6799 - ETA: 6803 - ETA: 6817 - ETA: 6827 - ETA: 6836 - ETA: 6844 - ETA: 6849 - ETA: 6859 - ETA: 6858 - ETA: 6863 - ETA: 6867 - ETA: 6871 - ETA: 6881 - ETA: 6884 - ETA: 6889 - ETA: 6893 - ETA: 6892 - ETA: 6897 - ETA: 6897 - ETA: 6902 - ETA: 6907 - ETA: 6910 - ETA: 6914 - ETA: 6918 - ETA: 6911 - ETA: 6912 - ETA: 6927 - ETA: 6924 - ETA: 6911 - ETA: 6915 - ETA: 6913 - ETA: 6908 - ETA: 6912 - ETA: 6908 - ETA: 6925 - ETA: 6916 - ETA: 6911 - ETA: 6916 - ETA: 6914 - ETA: 6915 - ETA: 6918 - ETA: 6913 - ETA: 6918 - ETA: 6922 - ETA: 6922 - ETA: 6931 - ETA: 6925 - ETA: 6928 - ETA: 6926 - ETA: 6943 - ETA: 6942 - ETA: 6946 - ETA: 6955 - ETA: 6958 - ETA: 6958 - ETA: 6963 - ETA: 6967 - ETA: 6970 - ETA: 6974 - ETA: 6977 - ETA: 6982 - ETA: 6986 - ETA: 6984 - ETA: 6988 - ETA: 6988 - ETA: 6993 - ETA: 6996 - ETA: 7000 - ETA: 7004 - ETA: 6999 - ETA: 6998 - ETA: 6997 - ETA: 6998 - ETA: 7002 - ETA: 7006 - ETA: 7001 - ETA: 7001 - ETA: 6995 - ETA: 6996 - ETA: 6986 - ETA: 6987 - ETA: 6982 - ETA: 6982 - ETA: 6968 - ETA: 6960 - ETA: 6949 - ETA: 6942 - ETA: 6928 - ETA: 6921 - ETA: 6902 - ETA: 6899 - ETA: 6852 - ETA: 6847 - ETA: 6835 - ETA: 6820 - ETA: 6792 - ETA: 6767 - ETA: 6747 - ETA: 6715 - ETA: 6688 - ETA: 6656 - ETA: 6660 - ETA: 6630 - ETA: 6592 - ETA: 6581 - ETA: 6569 - ETA: 6547 - ETA: 6529 - ETA: 6515 - ETA: 6489 - ETA: 6475 - ETA: 6464 - ETA: 6454 - ETA: 6440 - ETA: 6430 - ETA: 6414 - ETA: 6402 - ETA: 6399 - ETA: 6390 - ETA: 6390 - ETA: 6365 - ETA: 6362 - ETA: 6350 - ETA: 6340 - ETA: 6336 - ETA: 6324 - ETA: 6315 - ETA: 6311 - ETA: 6298 - ETA: 6289 - ETA: 6277 - ETA: 6268 - ETA: 6266 - ETA: 6253 - ETA: 6244 - ETA: 6236 - ETA: 6224 - ETA: 6215 - ETA: 6211 - ETA: 6199 - ETA: 6191 - ETA: 6183 - ETA: 6184 - ETA: 6168 - ETA: 6161 - ETA: 6155 - ETA: 6147 - ETA: 6143 - ETA: 6132 - ETA: 6130 - ETA: 6122 - ETA: 6131 - ETA: 6113 - ETA: 6121 - ETA: 6119 - ETA: 6120 - ETA: 6111 - ETA: 6125 - ETA: 6121 - ETA: 6122 - ETA: 6127 - ETA: 6130 - ETA: 6134 - ETA: 6136 - ETA: 6139 - ETA: 6136 - ETA: 6140 - ETA: 6141 - ETA: 6145 - ETA: 6148 - ETA: 6164 - ETA: 6162 - ETA: 6161 - ETA: 6175 - ETA: 6173 - ETA: 6179 - ETA: 6182 - ETA: 6189 - ETA: 6193 - ETA: 6203 - ETA: 6205 - ETA: 6209 - ETA: 6213 - ETA: 6214 - ETA: 6216 - ETA: 6221 - ETA: 6224 - ETA: 6228 - ETA: 6230 - ETA: 6235 - ETA: 6238 - ETA: 6239 - ETA: 6237 - ETA: 6236 - ETA: 6239 - ETA: 6236 - ETA: 6235 - ETA: 6239 - ETA: 6237 - ETA: 6233 - ETA: 6236 - ETA: 6236 - ETA: 6234 - ETA: 6238 - ETA: 6238 - ETA: 6236 - ETA: 6239 - ETA: 6236 - ETA: 6234s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 34332672/553467096 [>.............................] - ETA: 6237 - ETA: 6235 - ETA: 6235 - ETA: 6232 - ETA: 6236 - ETA: 6232 - ETA: 6232 - ETA: 6234 - ETA: 6232 - ETA: 6230 - ETA: 6229 - ETA: 6230 - ETA: 6226 - ETA: 6227 - ETA: 6224 - ETA: 6226 - ETA: 6222 - ETA: 6224 - ETA: 6221 - ETA: 6221 - ETA: 6219 - ETA: 6217 - ETA: 6217 - ETA: 6214 - ETA: 6214 - ETA: 6209 - ETA: 6210 - ETA: 6207 - ETA: 6208 - ETA: 6205 - ETA: 6208 - ETA: 6206 - ETA: 6201 - ETA: 6199 - ETA: 6197 - ETA: 6200 - ETA: 6192 - ETA: 6190 - ETA: 6193 - ETA: 6191 - ETA: 6192 - ETA: 6190 - ETA: 6192 - ETA: 6187 - ETA: 6188 - ETA: 6186 - ETA: 6188 - ETA: 6186 - ETA: 6180 - ETA: 6190 - ETA: 6182 - ETA: 6185 - ETA: 6181 - ETA: 6184 - ETA: 6186 - ETA: 6180 - ETA: 6183 - ETA: 6184 - ETA: 6181 - ETA: 6184 - ETA: 6186 - ETA: 6182 - ETA: 6184 - ETA: 6181 - ETA: 6180 - ETA: 6182 - ETA: 6182 - ETA: 6181 - ETA: 6183 - ETA: 6180 - ETA: 6180 - ETA: 6186 - ETA: 6186 - ETA: 6185 - ETA: 6182 - ETA: 6185 - ETA: 6181 - ETA: 6179 - ETA: 6181 - ETA: 6179 - ETA: 6178 - ETA: 6182 - ETA: 6174 - ETA: 6176 - ETA: 6179 - ETA: 6184 - ETA: 6181 - ETA: 6179 - ETA: 6182 - ETA: 6193 - ETA: 6187 - ETA: 6190 - ETA: 6193 - ETA: 6197 - ETA: 6201 - ETA: 6204 - ETA: 6205 - ETA: 6208 - ETA: 6208 - ETA: 6212 - ETA: 6215 - ETA: 6218 - ETA: 6221 - ETA: 6224 - ETA: 6226 - ETA: 6223 - ETA: 6223 - ETA: 6226 - ETA: 6229 - ETA: 6232 - ETA: 6235 - ETA: 6238 - ETA: 6236 - ETA: 6236 - ETA: 6238 - ETA: 6237 - ETA: 6240 - ETA: 6243 - ETA: 6240 - ETA: 6236 - ETA: 6236 - ETA: 6242 - ETA: 6240 - ETA: 6243 - ETA: 6246 - ETA: 6243 - ETA: 6241 - ETA: 6244 - ETA: 6246 - ETA: 6247 - ETA: 6245 - ETA: 6247 - ETA: 6249 - ETA: 6248 - ETA: 6246 - ETA: 6247 - ETA: 6249 - ETA: 6246 - ETA: 6248 - ETA: 6251 - ETA: 6247 - ETA: 6249 - ETA: 6247 - ETA: 6247 - ETA: 6245 - ETA: 6247 - ETA: 6247 - ETA: 6245 - ETA: 6246 - ETA: 6245 - ETA: 6243 - ETA: 6245 - ETA: 6244 - ETA: 6246 - ETA: 6244 - ETA: 6244 - ETA: 6240 - ETA: 6241 - ETA: 6239 - ETA: 6235 - ETA: 6232 - ETA: 6227 - ETA: 6227 - ETA: 6218 - ETA: 6215 - ETA: 6211 - ETA: 6208 - ETA: 6192 - ETA: 6197 - ETA: 6163 - ETA: 6156 - ETA: 6141 - ETA: 6139 - ETA: 6130 - ETA: 6125 - ETA: 6111 - ETA: 6109 - ETA: 6095 - ETA: 6082 - ETA: 6079 - ETA: 6076 - ETA: 6055 - ETA: 6058 - ETA: 6056 - ETA: 6059 - ETA: 6058 - ETA: 6053 - ETA: 6051 - ETA: 6046 - ETA: 6042 - ETA: 6033 - ETA: 6031 - ETA: 6027 - ETA: 6018 - ETA: 6013 - ETA: 6007 - ETA: 6002 - ETA: 5994 - ETA: 5989 - ETA: 5984 - ETA: 5979 - ETA: 5974 - ETA: 5967 - ETA: 5965 - ETA: 5956 - ETA: 5954 - ETA: 5943 - ETA: 5942 - ETA: 5938 - ETA: 5940 - ETA: 5920 - ETA: 5915 - ETA: 5914 - ETA: 5913 - ETA: 5914 - ETA: 5904 - ETA: 5903 - ETA: 5899 - ETA: 5898 - ETA: 5894 - ETA: 5889 - ETA: 5889 - ETA: 5887 - ETA: 5879 - ETA: 5886 - ETA: 5872 - ETA: 5872 - ETA: 5871 - ETA: 5868 - ETA: 5874 - ETA: 5865 - ETA: 5864 - ETA: 5864 - ETA: 5866 - ETA: 5863 - ETA: 5862 - ETA: 5861 - ETA: 5864 - ETA: 5870 - ETA: 5868 - ETA: 5865 - ETA: 5864 - ETA: 5866 - ETA: 5866 - ETA: 5869 - ETA: 5874 - ETA: 5873 - ETA: 5876 - ETA: 5877 - ETA: 5879 - ETA: 5881 - ETA: 5884 - ETA: 5884 - ETA: 5886 - ETA: 5889 - ETA: 5888 - ETA: 5889 - ETA: 5891 - ETA: 5894 - ETA: 5896 - ETA: 5903 - ETA: 5901 - ETA: 5910 - ETA: 5910 - ETA: 5916 - ETA: 5918 - ETA: 5924 - ETA: 5927 - ETA: 5929 - ETA: 5932 - ETA: 5930 - ETA: 5935 - ETA: 5939 - ETA: 5941 - ETA: 5944 - ETA: 5947 - ETA: 5952 - ETA: 5954 - ETA: 5957 - ETA: 5957 - ETA: 5959 - ETA: 5961 - ETA: 5969 - ETA: 5971 - ETA: 5973 - ETA: 5972 - ETA: 5976 - ETA: 5979 - ETA: 5979 - ETA: 5982 - ETA: 5984 - ETA: 5986 - ETA: 5996 - ETA: 5994 - ETA: 5996 - ETA: 5998 - ETA: 5999 - ETA: 6001 - ETA: 6003 - ETA: 6006 - ETA: 6009 - ETA: 6011 - ETA: 6013 - ETA: 6015 - ETA: 6017 - ETA: 6019 - ETA: 6021 - ETA: 6023 - ETA: 6026 - ETA: 6028 - ETA: 6035 - ETA: 6028 - ETA: 6031 - ETA: 6033 - ETA: 6036 - ETA: 6038 - ETA: 6047 - ETA: 6051 - ETA: 6055 - ETA: 6062 - ETA: 6069 - ETA: 6077 - ETA: 6082 - ETA: 6091 - ETA: 6099 - ETA: 6115 - ETA: 6128 - ETA: 6134 - ETA: 6140 - ETA: 6149 - ETA: 6155 - ETA: 6160 - ETA: 6166 - ETA: 6177 - ETA: 6179 - ETA: 6191 - ETA: 6201 - ETA: 6210 - ETA: 6218 - ETA: 6224 - ETA: 6239 - ETA: 6250 - ETA: 6263 - ETA: 6261 - ETA: 6272 - ETA: 6274 - ETA: 6282 - ETA: 6291 - ETA: 6293 - ETA: 6302 - ETA: 6304 - ETA: 6310 - ETA: 6313 - ETA: 6315 - ETA: 6318 - ETA: 6323 - ETA: 6325 - ETA: 6325 - ETA: 6325 - ETA: 6324 - ETA: 6327 - ETA: 6322 - ETA: 6321 - ETA: 6323 - ETA: 6322 - ETA: 6322 - ETA: 6323 - ETA: 6319 - ETA: 6322 - ETA: 6320 - ETA: 6318 - ETA: 6317 - ETA: 6319 - ETA: 6315 - ETA: 6317 - ETA: 6315 - ETA: 6314 - ETA: 6316 - ETA: 6312 - ETA: 6310 - ETA: 6309 - ETA: 6308 - ETA: 6310 - ETA: 6306 - ETA: 6303 - ETA: 6304 - ETA: 6302 - ETA: 6302 - ETA: 6301 - ETA: 6302 - ETA: 6302 - ETA: 6298 - ETA: 6298 - ETA: 6296 - ETA: 6296 - ETA: 6295 - ETA: 6295 - ETA: 6293 - ETA: 6293 - ETA: 6289 - ETA: 6289 - ETA: 6288 - ETA: 6289 - ETA: 6286 - ETA: 6287 - ETA: 6284 - ETA: 6285 - ETA: 6280 - ETA: 6281 - ETA: 6278 - ETA: 6279 - ETA: 6277 - ETA: 6278 - ETA: 6276 - ETA: 6271 - ETA: 6268 - ETA: 6270 - ETA: 6267 - ETA: 6268 - ETA: 6264 - ETA: 6260 - ETA: 6260 - ETA: 6258 - ETA: 6258 - ETA: 6256 - ETA: 6256 - ETA: 6254 - ETA: 6254 - ETA: 6251 - ETA: 6247 - ETA: 6245 - ETA: 6243 - ETA: 6236 - ETA: 6233 - ETA: 6231 - ETA: 6227 - ETA: 6225 - ETA: 6219 - ETA: 6218 - ETA: 6214 - ETA: 6204 - ETA: 6203 - ETA: 6197 - ETA: 6199 - ETA: 6172 - ETA: 6168 - ETA: 6166 - ETA: 6159 - ETA: 6156 - ETA: 6148 - ETA: 6142 - ETA: 6135 - ETA: 6127 - ETA: 6116 - ETA: 6114 - ETA: 6102 - ETA: 6102 - ETA: 6096 - ETA: 6086 - ETA: 6078 - ETA: 6068 - ETA: 6065 - ETA: 6050 - ETA: 6050 - ETA: 6045 - ETA: 6036 - ETA: 6030 - ETA: 6026 - ETA: 6006 - ETA: 6003 - ETA: 6001 - ETA: 5999 - ETA: 5987 - ETA: 5989 - ETA: 5979 - ETA: 5971 - ETA: 5965 - ETA: 5963 - ETA: 5951 - ETA: 5952 - ETA: 5949 - ETA: 5940 - ETA: 5941 - ETA: 5930 - ETA: 5924 - ETA: 5918 - ETA: 5910 - ETA: 5904 - ETA: 5902 - ETA: 5896 - ETA: 5891 - ETA: 5889 - ETA: 5883 - ETA: 5880 - ETA: 5878 - ETA: 5870 - ETA: 5864 - ETA: 5859 - ETA: 5851 - ETA: 5845 - ETA: 5846 - ETA: 5840 - ETA: 5838 - ETA: 5815 - ETA: 5812 - ETA: 5815 - ETA: 5804 - ETA: 5799 - ETA: 5792 - ETA: 5786 - ETA: 5787 - ETA: 5785 - ETA: 5783 - ETA: 5781 - ETA: 5779 - ETA: 5777 - ETA: 5775 - ETA: 5773 - ETA: 5773 - ETA: 5771 - ETA: 5770 - ETA: 5766 - ETA: 5764 - ETA: 5761 - ETA: 5762 - ETA: 5760 - ETA: 5758 - ETA: 5756 - ETA: 5754 - ETA: 5752 - ETA: 5750 - ETA: 5750 - ETA: 5748 - ETA: 5747 - ETA: 5744 - ETA: 5741 - ETA: 5739 - ETA: 5736 - ETA: 5734 - ETA: 5732 - ETA: 5729 - ETA: 5727 - ETA: 5725 - ETA: 5723 - ETA: 5721 - ETA: 5716 - ETA: 5713 - ETA: 5711 - ETA: 5709 - ETA: 5703 - ETA: 5703 - ETA: 5701 - ETA: 5699 - ETA: 5697 - ETA: 5698 - ETA: 5687 - ETA: 5688 - ETA: 5686 - ETA: 5686 - ETA: 5683 - ETA: 5686 - ETA: 5685 - ETA: 5678 - ETA: 5678 - ETA: 5679 - ETA: 5677 - ETA: 5682 - ETA: 5678 - ETA: 5680 - ETA: 5681 - ETA: 5678 - ETA: 5680 - ETA: 5681 - ETA: 5682 - ETA: 5681 - ETA: 5682 - ETA: 5684 - ETA: 5683 - ETA: 5682 - ETA: 5683 - ETA: 5685 - ETA: 5684 - ETA: 5684 - ETA: 5686 - ETA: 5685 - ETA: 5685 - ETA: 5687 - ETA: 5684 - ETA: 5684 - ETA: 5686 - ETA: 5686 - ETA: 5685 - ETA: 5687 - ETA: 5687 - ETA: 5686 - ETA: 5685 - ETA: 5684 - ETA: 5685 - ETA: 5685 - ETA: 5684 - ETA: 5691 - ETA: 5690 - ETA: 5687 - ETA: 5689 - ETA: 5689 - ETA: 5691 - ETA: 5693 - ETA: 5695 - ETA: 5697 - ETA: 5695 - ETA: 5697 - ETA: 5698 - ETA: 5700 - ETA: 5700 - ETA: 5702 - ETA: 5701 - ETA: 5703 - ETA: 5703 - ETA: 5703 - ETA: 5705 - ETA: 5707 - ETA: 5706 - ETA: 5707 - ETA: 5713 - ETA: 5711 - ETA: 5710 - ETA: 5712 - ETA: 5713 - ETA: 5713 - ETA: 5716 - ETA: 5718 - ETA: 5720 - ETA: 5718 - ETA: 5719 - ETA: 5719 - ETA: 5725 - ETA: 5727 - ETA: 5729 - ETA: 5731 - ETA: 5733 - ETA: 5735 - ETA: 5735 - ETA: 5737 - ETA: 5739 - ETA: 5738 - ETA: 5739 - ETA: 5741 - ETA: 5743 - ETA: 5743 - ETA: 5742 - ETA: 5744 - ETA: 5745 - ETA: 5747 - ETA: 5749 - ETA: 5748 - ETA: 5750 - ETA: 5750 - ETA: 5753 - ETA: 5757 - ETA: 5755 - ETA: 5757 - ETA: 5762 - ETA: 5764 - ETA: 5765 - ETA: 5766 - ETA: 5769 - ETA: 5769 - ETA: 5771 - ETA: 5773 - ETA: 5776 - ETA: 5778 - ETA: 5779 - ETA: 5780 - ETA: 5780 - ETA: 5778 - ETA: 5780 - ETA: 5782 - ETA: 5784 - ETA: 5785s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 53600256/553467096 [=>............................] - ETA: 5787 - ETA: 5786 - ETA: 5786 - ETA: 5789 - ETA: 5785 - ETA: 5787 - ETA: 5785 - ETA: 5790 - ETA: 5786 - ETA: 5787 - ETA: 5788 - ETA: 5788 - ETA: 5795 - ETA: 5793 - ETA: 5796 - ETA: 5797 - ETA: 5798 - ETA: 5798 - ETA: 5800 - ETA: 5802 - ETA: 5800 - ETA: 5801 - ETA: 5802 - ETA: 5808 - ETA: 5803 - ETA: 5806 - ETA: 5811 - ETA: 5813 - ETA: 5815 - ETA: 5821 - ETA: 5828 - ETA: 5835 - ETA: 5840 - ETA: 5845 - ETA: 5852 - ETA: 5862 - ETA: 5872 - ETA: 5879 - ETA: 5885 - ETA: 5892 - ETA: 5896 - ETA: 5898 - ETA: 5903 - ETA: 5906 - ETA: 5908 - ETA: 5912 - ETA: 5914 - ETA: 5917 - ETA: 5919 - ETA: 5919 - ETA: 5924 - ETA: 5919 - ETA: 5921 - ETA: 5922 - ETA: 5922 - ETA: 5923 - ETA: 5921 - ETA: 5922 - ETA: 5924 - ETA: 5924 - ETA: 5923 - ETA: 5923 - ETA: 5925 - ETA: 5926 - ETA: 5922 - ETA: 5924 - ETA: 5925 - ETA: 5925 - ETA: 5923 - ETA: 5928 - ETA: 5927 - ETA: 5928 - ETA: 5928 - ETA: 5929 - ETA: 5935 - ETA: 5933 - ETA: 5936 - ETA: 5938 - ETA: 5939 - ETA: 5938 - ETA: 5940 - ETA: 5942 - ETA: 5943 - ETA: 5944 - ETA: 5946 - ETA: 5945 - ETA: 5947 - ETA: 5947 - ETA: 5947 - ETA: 5949 - ETA: 5951 - ETA: 5952 - ETA: 5954 - ETA: 5956 - ETA: 5957 - ETA: 5957 - ETA: 5959 - ETA: 5960 - ETA: 5962 - ETA: 5964 - ETA: 5965 - ETA: 5965 - ETA: 5965 - ETA: 5964 - ETA: 5965 - ETA: 5966 - ETA: 5968 - ETA: 5969 - ETA: 5970 - ETA: 5970 - ETA: 5976 - ETA: 5974 - ETA: 5975 - ETA: 5977 - ETA: 5979 - ETA: 5980 - ETA: 5981 - ETA: 5984 - ETA: 5987 - ETA: 5986 - ETA: 5988 - ETA: 5990 - ETA: 5994 - ETA: 5995 - ETA: 5997 - ETA: 5999 - ETA: 6000 - ETA: 6000 - ETA: 6002 - ETA: 6004 - ETA: 6005 - ETA: 6006 - ETA: 6008 - ETA: 6010 - ETA: 6011 - ETA: 6011 - ETA: 6010 - ETA: 6012 - ETA: 6013 - ETA: 6013 - ETA: 6014 - ETA: 6014 - ETA: 6013 - ETA: 6010 - ETA: 6010 - ETA: 6009 - ETA: 6006 - ETA: 6005 - ETA: 6002 - ETA: 5997 - ETA: 5996 - ETA: 5989 - ETA: 5986 - ETA: 5978 - ETA: 5975 - ETA: 5965 - ETA: 5958 - ETA: 5949 - ETA: 5942 - ETA: 5932 - ETA: 5926 - ETA: 5923 - ETA: 5911 - ETA: 5901 - ETA: 5887 - ETA: 5876 - ETA: 5862 - ETA: 5855 - ETA: 5848 - ETA: 5834 - ETA: 5832 - ETA: 5792 - ETA: 5782 - ETA: 5772 - ETA: 5772 - ETA: 5761 - ETA: 5760 - ETA: 5758 - ETA: 5725 - ETA: 5724 - ETA: 5715 - ETA: 5715 - ETA: 5711 - ETA: 5707 - ETA: 5701 - ETA: 5696 - ETA: 5686 - ETA: 5678 - ETA: 5669 - ETA: 5661 - ETA: 5652 - ETA: 5644 - ETA: 5634 - ETA: 5627 - ETA: 5617 - ETA: 5609 - ETA: 5600 - ETA: 5593 - ETA: 5581 - ETA: 5576 - ETA: 5564 - ETA: 5559 - ETA: 5548 - ETA: 5543 - ETA: 5534 - ETA: 5518 - ETA: 5499 - ETA: 5495 - ETA: 5483 - ETA: 5481 - ETA: 5470 - ETA: 5471 - ETA: 5442 - ETA: 5435 - ETA: 5429 - ETA: 5417 - ETA: 5416 - ETA: 5393 - ETA: 5392 - ETA: 5393 - ETA: 5389 - ETA: 5391 - ETA: 5381 - ETA: 5374 - ETA: 5375 - ETA: 5364 - ETA: 5361 - ETA: 5357 - ETA: 5355 - ETA: 5354 - ETA: 5353 - ETA: 5349 - ETA: 5347 - ETA: 5346 - ETA: 5341 - ETA: 5340 - ETA: 5338 - ETA: 5340 - ETA: 5337 - ETA: 5337 - ETA: 5336 - ETA: 5335 - ETA: 5334 - ETA: 5331 - ETA: 5330 - ETA: 5328 - ETA: 5328 - ETA: 5329 - ETA: 5326 - ETA: 5326 - ETA: 5322 - ETA: 5321 - ETA: 5321 - ETA: 5320 - ETA: 5317 - ETA: 5316 - ETA: 5316 - ETA: 5313 - ETA: 5313 - ETA: 5312 - ETA: 5308 - ETA: 5308 - ETA: 5307 - ETA: 5304 - ETA: 5305 - ETA: 5303 - ETA: 5300 - ETA: 5300 - ETA: 5297 - ETA: 5297 - ETA: 5295 - ETA: 5292 - ETA: 5293 - ETA: 5292 - ETA: 5288 - ETA: 5288 - ETA: 5285 - ETA: 5285 - ETA: 5286 - ETA: 5281 - ETA: 5280 - ETA: 5280 - ETA: 5280 - ETA: 5279 - ETA: 5276 - ETA: 5277 - ETA: 5274 - ETA: 5275 - ETA: 5274 - ETA: 5272 - ETA: 5270 - ETA: 5269 - ETA: 5270 - ETA: 5269 - ETA: 5269 - ETA: 5270 - ETA: 5265 - ETA: 5265 - ETA: 5264 - ETA: 5264 - ETA: 5263 - ETA: 5263 - ETA: 5263 - ETA: 5263 - ETA: 5263 - ETA: 5262 - ETA: 5262 - ETA: 5263 - ETA: 5260 - ETA: 5260 - ETA: 5261 - ETA: 5261 - ETA: 5261 - ETA: 5262 - ETA: 5260 - ETA: 5260 - ETA: 5260 - ETA: 5260 - ETA: 5261 - ETA: 5261 - ETA: 5261 - ETA: 5260 - ETA: 5259 - ETA: 5259 - ETA: 5260 - ETA: 5260 - ETA: 5259 - ETA: 5259 - ETA: 5258 - ETA: 5258 - ETA: 5257 - ETA: 5257 - ETA: 5258 - ETA: 5258 - ETA: 5257 - ETA: 5257 - ETA: 5257 - ETA: 5257 - ETA: 5257 - ETA: 5256 - ETA: 5256 - ETA: 5256 - ETA: 5256 - ETA: 5256 - ETA: 5259 - ETA: 5256 - ETA: 5255 - ETA: 5255 - ETA: 5256 - ETA: 5256 - ETA: 5256 - ETA: 5257 - ETA: 5257 - ETA: 5256 - ETA: 5256 - ETA: 5256 - ETA: 5256 - ETA: 5256 - ETA: 5256 - ETA: 5257 - ETA: 5257 - ETA: 5256 - ETA: 5256 - ETA: 5256 - ETA: 5255 - ETA: 5255 - ETA: 5255 - ETA: 5255 - ETA: 5256 - ETA: 5256 - ETA: 5254 - ETA: 5254 - ETA: 5254 - ETA: 5254 - ETA: 5253 - ETA: 5256 - ETA: 5257 - ETA: 5254 - ETA: 5254 - ETA: 5255 - ETA: 5255 - ETA: 5255 - ETA: 5256 - ETA: 5256 - ETA: 5256 - ETA: 5256 - ETA: 5255 - ETA: 5255 - ETA: 5256 - ETA: 5256 - ETA: 5256 - ETA: 5256 - ETA: 5255 - ETA: 5255 - ETA: 5256 - ETA: 5256 - ETA: 5259 - ETA: 5256 - ETA: 5257 - ETA: 5258 - ETA: 5259 - ETA: 5258 - ETA: 5259 - ETA: 5259 - ETA: 5260 - ETA: 5260 - ETA: 5261 - ETA: 5264 - ETA: 5261 - ETA: 5262 - ETA: 5264 - ETA: 5265 - ETA: 5265 - ETA: 5266 - ETA: 5267 - ETA: 5267 - ETA: 5268 - ETA: 5268 - ETA: 5268 - ETA: 5273 - ETA: 5272 - ETA: 5274 - ETA: 5275 - ETA: 5276 - ETA: 5278 - ETA: 5280 - ETA: 5281 - ETA: 5283 - ETA: 5284 - ETA: 5287 - ETA: 5288 - ETA: 5287 - ETA: 5289 - ETA: 5290 - ETA: 5291 - ETA: 5297 - ETA: 5296 - ETA: 5298 - ETA: 5303 - ETA: 5303 - ETA: 5304 - ETA: 5312 - ETA: 5313 - ETA: 5314 - ETA: 5316 - ETA: 5317 - ETA: 5318 - ETA: 5321 - ETA: 5322 - ETA: 5323 - ETA: 5324 - ETA: 5326 - ETA: 5327 - ETA: 5328 - ETA: 5330 - ETA: 5330 - ETA: 5331 - ETA: 5336 - ETA: 5335 - ETA: 5336 - ETA: 5338 - ETA: 5340 - ETA: 5340 - ETA: 5341 - ETA: 5341 - ETA: 5342 - ETA: 5343 - ETA: 5348 - ETA: 5346 - ETA: 5347 - ETA: 5349 - ETA: 5350 - ETA: 5355 - ETA: 5355 - ETA: 5357 - ETA: 5358 - ETA: 5359 - ETA: 5360 - ETA: 5361 - ETA: 5362 - ETA: 5364 - ETA: 5365 - ETA: 5368 - ETA: 5370 - ETA: 5368 - ETA: 5369 - ETA: 5370 - ETA: 5372 - ETA: 5373 - ETA: 5374 - ETA: 5375 - ETA: 5380 - ETA: 5378 - ETA: 5379 - ETA: 5381 - ETA: 5382 - ETA: 5383 - ETA: 5388 - ETA: 5386 - ETA: 5388 - ETA: 5390 - ETA: 5392 - ETA: 5393 - ETA: 5394 - ETA: 5395 - ETA: 5397 - ETA: 5400 - ETA: 5401 - ETA: 5403 - ETA: 5406 - ETA: 5408 - ETA: 5409 - ETA: 5411 - ETA: 5412 - ETA: 5413 - ETA: 5414 - ETA: 5415 - ETA: 5416 - ETA: 5417 - ETA: 5419 - ETA: 5420 - ETA: 5421 - ETA: 5422 - ETA: 5423 - ETA: 5424 - ETA: 5425 - ETA: 5426 - ETA: 5427 - ETA: 5428 - ETA: 5429 - ETA: 5428 - ETA: 5428 - ETA: 5429 - ETA: 5430 - ETA: 5429 - ETA: 5430 - ETA: 5431 - ETA: 5431 - ETA: 5427 - ETA: 5427 - ETA: 5427 - ETA: 5428 - ETA: 5427 - ETA: 5428 - ETA: 5426 - ETA: 5425 - ETA: 5428 - ETA: 5427 - ETA: 5428 - ETA: 5429 - ETA: 5429 - ETA: 5429 - ETA: 5430 - ETA: 5431 - ETA: 5431 - ETA: 5432 - ETA: 5433 - ETA: 5433 - ETA: 5432 - ETA: 5433 - ETA: 5434 - ETA: 5434 - ETA: 5434 - ETA: 5436 - ETA: 5434 - ETA: 5435 - ETA: 5435 - ETA: 5436 - ETA: 5437 - ETA: 5436 - ETA: 5435 - ETA: 5434 - ETA: 5433 - ETA: 5431 - ETA: 5430 - ETA: 5428 - ETA: 5426 - ETA: 5424 - ETA: 5422 - ETA: 5413 - ETA: 5412 - ETA: 5408 - ETA: 5404 - ETA: 5397 - ETA: 5396 - ETA: 5393 - ETA: 5391 - ETA: 5386 - ETA: 5381 - ETA: 5378 - ETA: 5375 - ETA: 5374 - ETA: 5368 - ETA: 5368 - ETA: 5358 - ETA: 5356 - ETA: 5358 - ETA: 5352 - ETA: 5348 - ETA: 5346 - ETA: 5344 - ETA: 5342 - ETA: 5341 - ETA: 5338 - ETA: 5338 - ETA: 5337 - ETA: 5334 - ETA: 5333 - ETA: 5332 - ETA: 5331 - ETA: 5332 - ETA: 5326 - ETA: 5326 - ETA: 5325 - ETA: 5323 - ETA: 5322 - ETA: 5320 - ETA: 5319 - ETA: 5319 - ETA: 5317 - ETA: 5315 - ETA: 5315 - ETA: 5312 - ETA: 5311 - ETA: 5308 - ETA: 5307 - ETA: 5305 - ETA: 5305 - ETA: 5302 - ETA: 5301 - ETA: 5301 - ETA: 5298 - ETA: 5296 - ETA: 5295 - ETA: 5294 - ETA: 5292 - ETA: 5288 - ETA: 5287 - ETA: 5286 - ETA: 5284 - ETA: 5284 - ETA: 5283 - ETA: 5282 - ETA: 5279 - ETA: 5278 - ETA: 5278 - ETA: 5276 - ETA: 5275 - ETA: 5275 - ETA: 5274 - ETA: 5273 - ETA: 5273 - ETA: 5270 - ETA: 5270 - ETA: 5267 - ETA: 5265 - ETA: 5262 - ETA: 5262 - ETA: 5255 - ETA: 5254 - ETA: 5251 - ETA: 5252 - ETA: 5254 - ETA: 5249 - ETA: 5248 - ETA: 5250 - ETA: 5252 - ETA: 5251 - ETA: 5248 - ETA: 5248 - ETA: 5248 - ETA: 5248 - ETA: 5249 - ETA: 5249 - ETA: 5248 - ETA: 5249 - ETA: 5250 - ETA: 5250 - ETA: 5251s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 67682304/553467096 [==>...........................] - ETA: 5250 - ETA: 5249 - ETA: 5250 - ETA: 5251 - ETA: 5251 - ETA: 5250 - ETA: 5251 - ETA: 5252 - ETA: 5251 - ETA: 5252 - ETA: 5251 - ETA: 5251 - ETA: 5255 - ETA: 5253 - ETA: 5253 - ETA: 5252 - ETA: 5253 - ETA: 5254 - ETA: 5255 - ETA: 5255 - ETA: 5255 - ETA: 5255 - ETA: 5256 - ETA: 5256 - ETA: 5255 - ETA: 5256 - ETA: 5256 - ETA: 5256 - ETA: 5256 - ETA: 5256 - ETA: 5256 - ETA: 5256 - ETA: 5256 - ETA: 5257 - ETA: 5257 - ETA: 5257 - ETA: 5258 - ETA: 5259 - ETA: 5257 - ETA: 5258 - ETA: 5261 - ETA: 5261 - ETA: 5262 - ETA: 5263 - ETA: 5264 - ETA: 5266 - ETA: 5266 - ETA: 5267 - ETA: 5270 - ETA: 5275 - ETA: 5275 - ETA: 5279 - ETA: 5283 - ETA: 5287 - ETA: 5286 - ETA: 5293 - ETA: 5300 - ETA: 5304 - ETA: 5310 - ETA: 5318 - ETA: 5325 - ETA: 5331 - ETA: 5335 - ETA: 5336 - ETA: 5340 - ETA: 5344 - ETA: 5347 - ETA: 5350 - ETA: 5352 - ETA: 5355 - ETA: 5356 - ETA: 5359 - ETA: 5360 - ETA: 5361 - ETA: 5362 - ETA: 5363 - ETA: 5364 - ETA: 5365 - ETA: 5364 - ETA: 5364 - ETA: 5362 - ETA: 5362 - ETA: 5362 - ETA: 5363 - ETA: 5356 - ETA: 5356 - ETA: 5355 - ETA: 5355 - ETA: 5354 - ETA: 5353 - ETA: 5352 - ETA: 5351 - ETA: 5352 - ETA: 5350 - ETA: 5347 - ETA: 5347 - ETA: 5346 - ETA: 5346 - ETA: 5345 - ETA: 5345 - ETA: 5344 - ETA: 5345 - ETA: 5344 - ETA: 5347 - ETA: 5343 - ETA: 5342 - ETA: 5342 - ETA: 5342 - ETA: 5341 - ETA: 5340 - ETA: 5341 - ETA: 5343 - ETA: 5341 - ETA: 5340 - ETA: 5341 - ETA: 5342 - ETA: 5341 - ETA: 5341 - ETA: 5341 - ETA: 5342 - ETA: 5341 - ETA: 5340 - ETA: 5344 - ETA: 5342 - ETA: 5341 - ETA: 5341 - ETA: 5342 - ETA: 5345 - ETA: 5343 - ETA: 5344 - ETA: 5345 - ETA: 5346 - ETA: 5347 - ETA: 5347 - ETA: 5348 - ETA: 5349 - ETA: 5350 - ETA: 5350 - ETA: 5352 - ETA: 5352 - ETA: 5352 - ETA: 5355 - ETA: 5354 - ETA: 5353 - ETA: 5355 - ETA: 5356 - ETA: 5357 - ETA: 5358 - ETA: 5359 - ETA: 5360 - ETA: 5361 - ETA: 5361 - ETA: 5361 - ETA: 5361 - ETA: 5362 - ETA: 5363 - ETA: 5365 - ETA: 5366 - ETA: 5367 - ETA: 5368 - ETA: 5367 - ETA: 5367 - ETA: 5367 - ETA: 5368 - ETA: 5370 - ETA: 5369 - ETA: 5369 - ETA: 5370 - ETA: 5371 - ETA: 5371 - ETA: 5372 - ETA: 5371 - ETA: 5372 - ETA: 5373 - ETA: 5374 - ETA: 5376 - ETA: 5377 - ETA: 5376 - ETA: 5377 - ETA: 5378 - ETA: 5379 - ETA: 5380 - ETA: 5381 - ETA: 5380 - ETA: 5381 - ETA: 5381 - ETA: 5381 - ETA: 5382 - ETA: 5382 - ETA: 5383 - ETA: 5384 - ETA: 5384 - ETA: 5383 - ETA: 5383 - ETA: 5383 - ETA: 5387 - ETA: 5388 - ETA: 5387 - ETA: 5391 - ETA: 5393 - ETA: 5392 - ETA: 5394 - ETA: 5395 - ETA: 5398 - ETA: 5399 - ETA: 5400 - ETA: 5401 - ETA: 5403 - ETA: 5402 - ETA: 5404 - ETA: 5408 - ETA: 5407 - ETA: 5408 - ETA: 5411 - ETA: 5415 - ETA: 5421 - ETA: 5424 - ETA: 5427 - ETA: 5429 - ETA: 5432 - ETA: 5434 - ETA: 5437 - ETA: 5438 - ETA: 5439 - ETA: 5442 - ETA: 5442 - ETA: 5444 - ETA: 5445 - ETA: 5446 - ETA: 5447 - ETA: 5449 - ETA: 5450 - ETA: 5449 - ETA: 5448 - ETA: 5447 - ETA: 5446 - ETA: 5444 - ETA: 5443 - ETA: 5442 - ETA: 5439 - ETA: 5438 - ETA: 5437 - ETA: 5433 - ETA: 5432 - ETA: 5427 - ETA: 5425 - ETA: 5423 - ETA: 5421 - ETA: 5420 - ETA: 5420 - ETA: 5418 - ETA: 5416 - ETA: 5415 - ETA: 5412 - ETA: 5411 - ETA: 5408 - ETA: 5407 - ETA: 5405 - ETA: 5404 - ETA: 5404 - ETA: 5401 - ETA: 5399 - ETA: 5398 - ETA: 5397 - ETA: 5398 - ETA: 5393 - ETA: 5392 - ETA: 5394 - ETA: 5389 - ETA: 5390 - ETA: 5389 - ETA: 5390 - ETA: 5390 - ETA: 5390 - ETA: 5389 - ETA: 5390 - ETA: 5388 - ETA: 5388 - ETA: 5390 - ETA: 5389 - ETA: 5390 - ETA: 5391 - ETA: 5389 - ETA: 5390 - ETA: 5389 - ETA: 5392 - ETA: 5391 - ETA: 5395 - ETA: 5395 - ETA: 5396 - ETA: 5397 - ETA: 5398 - ETA: 5400 - ETA: 5402 - ETA: 5403 - ETA: 5406 - ETA: 5407 - ETA: 5407 - ETA: 5413 - ETA: 5414 - ETA: 5417 - ETA: 5420 - ETA: 5424 - ETA: 5425 - ETA: 5428 - ETA: 5429 - ETA: 5432 - ETA: 5433 - ETA: 5434 - ETA: 5436 - ETA: 5438 - ETA: 5439 - ETA: 5440 - ETA: 5442 - ETA: 5446 - ETA: 5449 - ETA: 5457 - ETA: 5461 - ETA: 5468 - ETA: 5474 - ETA: 5478 - ETA: 5480 - ETA: 5484 - ETA: 5490 - ETA: 5491 - ETA: 5498 - ETA: 5502 - ETA: 5507 - ETA: 5509 - ETA: 5510 - ETA: 5514 - ETA: 5520 - ETA: 5525 - ETA: 5529 - ETA: 5533 - ETA: 5539 - ETA: 5545 - ETA: 5551 - ETA: 5554 - ETA: 5558 - ETA: 5564 - ETA: 5568 - ETA: 5573 - ETA: 5578 - ETA: 5584 - ETA: 5587 - ETA: 5590 - ETA: 5593 - ETA: 5599 - ETA: 5603 - ETA: 5608 - ETA: 5614 - ETA: 5620 - ETA: 5626 - ETA: 5630 - ETA: 5636 - ETA: 5640 - ETA: 5645 - ETA: 5654 - ETA: 5670 - ETA: 5677 - ETA: 5684 - ETA: 5684 - ETA: 5689 - ETA: 5692 - ETA: 5697 - ETA: 5700 - ETA: 5702 - ETA: 5705 - ETA: 5707 - ETA: 5710 - ETA: 5710 - ETA: 5712 - ETA: 5717 - ETA: 5721 - ETA: 5725 - ETA: 5730 - ETA: 5745 - ETA: 5755 - ETA: 5763 - ETA: 5773 - ETA: 5779 - ETA: 5787 - ETA: 5795 - ETA: 5803 - ETA: 5813 - ETA: 5819 - ETA: 5818 - ETA: 5824 - ETA: 5830 - ETA: 5836 - ETA: 5842 - ETA: 5848 - ETA: 5856 - ETA: 5862 - ETA: 5867 - ETA: 5866 - ETA: 5870 - ETA: 5873 - ETA: 5879 - ETA: 5882 - ETA: 5890 - ETA: 5894 - ETA: 5899 - ETA: 5902 - ETA: 5904 - ETA: 5906 - ETA: 5909 - ETA: 5912 - ETA: 5916 - ETA: 5921 - ETA: 5929 - ETA: 5937 - ETA: 5937 - ETA: 5943 - ETA: 5946 - ETA: 5951 - ETA: 5955 - ETA: 5963 - ETA: 5967 - ETA: 5975 - ETA: 6009 - ETA: 6019 - ETA: 6025 - ETA: 6030 - ETA: 6034 - ETA: 6039 - ETA: 6040 - ETA: 6045 - ETA: 6049 - ETA: 6049 - ETA: 6053 - ETA: 6057 - ETA: 6060 - ETA: 6063 - ETA: 6066 - ETA: 6071 - ETA: 6073 - ETA: 6080 - ETA: 6089 - ETA: 6095 - ETA: 6107 - ETA: 6113 - ETA: 6119 - ETA: 6123 - ETA: 6128 - ETA: 6130 - ETA: 6129 - ETA: 6132 - ETA: 6134 - ETA: 6136 - ETA: 6138 - ETA: 6140 - ETA: 6143 - ETA: 6148 - ETA: 6156 - ETA: 6161 - ETA: 6167 - ETA: 6170 - ETA: 6178 - ETA: 6189 - ETA: 6203 - ETA: 6213 - ETA: 6227 - ETA: 6226 - ETA: 6235 - ETA: 6248 - ETA: 6256 - ETA: 6263 - ETA: 6266 - ETA: 6269 - ETA: 6276 - ETA: 6279 - ETA: 6279 - ETA: 6286 - ETA: 6289 - ETA: 6297 - ETA: 6303 - ETA: 6307 - ETA: 6311 - ETA: 6316 - ETA: 6319 - ETA: 6325 - ETA: 6330 - ETA: 6335 - ETA: 6339 - ETA: 6341 - ETA: 6344 - ETA: 6348 - ETA: 6348 - ETA: 6350 - ETA: 6351 - ETA: 6354 - ETA: 6355 - ETA: 6358 - ETA: 6358 - ETA: 6361 - ETA: 6364 - ETA: 6364 - ETA: 6366 - ETA: 6369 - ETA: 6371 - ETA: 6373 - ETA: 6373 - ETA: 6379 - ETA: 6379 - ETA: 6385 - ETA: 6387 - ETA: 6390 - ETA: 6397 - ETA: 6402 - ETA: 6405 - ETA: 6409 - ETA: 6412 - ETA: 6415 - ETA: 6417 - ETA: 6423 - ETA: 6427 - ETA: 6431 - ETA: 6434 - ETA: 6436 - ETA: 6439 - ETA: 6441 - ETA: 6442 - ETA: 6443 - ETA: 6445 - ETA: 6446 - ETA: 6446 - ETA: 6448 - ETA: 6450 - ETA: 6450 - ETA: 6453 - ETA: 6451 - ETA: 6451 - ETA: 6450 - ETA: 6452 - ETA: 6453 - ETA: 6453 - ETA: 6454 - ETA: 6458 - ETA: 6457 - ETA: 6462 - ETA: 6467 - ETA: 6473 - ETA: 6478 - ETA: 6481 - ETA: 6484 - ETA: 6484 - ETA: 6488 - ETA: 6497 - ETA: 6505 - ETA: 6510 - ETA: 6515 - ETA: 6520 - ETA: 6526 - ETA: 6532 - ETA: 6534 - ETA: 6537 - ETA: 6539 - ETA: 6542 - ETA: 6545 - ETA: 6547 - ETA: 6549 - ETA: 6551 - ETA: 6551 - ETA: 6552 - ETA: 6554 - ETA: 6556 - ETA: 6556 - ETA: 6557 - ETA: 6557 - ETA: 6558 - ETA: 6556 - ETA: 6555 - ETA: 6555 - ETA: 6554 - ETA: 6555 - ETA: 6554 - ETA: 6553 - ETA: 6551 - ETA: 6547 - ETA: 6547 - ETA: 6548 - ETA: 6547 - ETA: 6547 - ETA: 6546 - ETA: 6547 - ETA: 6546 - ETA: 6545 - ETA: 6546 - ETA: 6545 - ETA: 6544 - ETA: 6544 - ETA: 6543 - ETA: 6543 - ETA: 6543 - ETA: 6544 - ETA: 6541 - ETA: 6542 - ETA: 6540 - ETA: 6540 - ETA: 6539 - ETA: 6540 - ETA: 6538 - ETA: 6537 - ETA: 6538 - ETA: 6537 - ETA: 6536 - ETA: 6537 - ETA: 6536 - ETA: 6534 - ETA: 6533 - ETA: 6535 - ETA: 6532 - ETA: 6533 - ETA: 6533 - ETA: 6532 - ETA: 6531 - ETA: 6532 - ETA: 6532 - ETA: 6532 - ETA: 6531 - ETA: 6532 - ETA: 6532 - ETA: 6532 - ETA: 6532 - ETA: 6531 - ETA: 6532 - ETA: 6531 - ETA: 6532 - ETA: 6532 - ETA: 6533 - ETA: 6531 - ETA: 6530 - ETA: 6531 - ETA: 6531 - ETA: 6530 - ETA: 6530 - ETA: 6530 - ETA: 6531 - ETA: 6529 - ETA: 6529 - ETA: 6529 - ETA: 6530 - ETA: 6529 - ETA: 6528 - ETA: 6529 - ETA: 6529 - ETA: 6527 - ETA: 6526 - ETA: 6526 - ETA: 6526 - ETA: 6525 - ETA: 6522 - ETA: 6519 - ETA: 6516 - ETA: 6512 - ETA: 6507 - ETA: 6508 - ETA: 6496 - ETA: 6490 - ETA: 6485 - ETA: 6480 - ETA: 6475 - ETA: 6469 - ETA: 6463 - ETA: 6462 - ETA: 6456 - ETA: 6451 - ETA: 6445 - ETA: 6441 - ETA: 6434 - ETA: 6433 - ETA: 6428 - ETA: 6422 - ETA: 6417 - ETA: 6410 - ETA: 6411 - ETA: 6408s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 84484096/553467096 [===>..........................] - ETA: 6407 - ETA: 6402 - ETA: 6395 - ETA: 6395 - ETA: 6393 - ETA: 6391 - ETA: 6390 - ETA: 6388 - ETA: 6385 - ETA: 6385 - ETA: 6382 - ETA: 6382 - ETA: 6380 - ETA: 6377 - ETA: 6377 - ETA: 6378 - ETA: 6373 - ETA: 6370 - ETA: 6369 - ETA: 6367 - ETA: 6366 - ETA: 6364 - ETA: 6364 - ETA: 6363 - ETA: 6363 - ETA: 6362 - ETA: 6360 - ETA: 6357 - ETA: 6357 - ETA: 6355 - ETA: 6354 - ETA: 6352 - ETA: 6349 - ETA: 6348 - ETA: 6345 - ETA: 6344 - ETA: 6345 - ETA: 6342 - ETA: 6340 - ETA: 6338 - ETA: 6337 - ETA: 6336 - ETA: 6335 - ETA: 6333 - ETA: 6331 - ETA: 6332 - ETA: 6333 - ETA: 6329 - ETA: 6327 - ETA: 6328 - ETA: 6329 - ETA: 6327 - ETA: 6328 - ETA: 6326 - ETA: 6328 - ETA: 6330 - ETA: 6330 - ETA: 6330 - ETA: 6331 - ETA: 6333 - ETA: 6334 - ETA: 6334 - ETA: 6336 - ETA: 6337 - ETA: 6337 - ETA: 6344 - ETA: 6354 - ETA: 6358 - ETA: 6366 - ETA: 6372 - ETA: 6379 - ETA: 6382 - ETA: 6386 - ETA: 6389 - ETA: 6392 - ETA: 6395 - ETA: 6402 - ETA: 6418 - ETA: 6428 - ETA: 6434 - ETA: 6439 - ETA: 6444 - ETA: 6448 - ETA: 6451 - ETA: 6456 - ETA: 6459 - ETA: 6463 - ETA: 6466 - ETA: 6467 - ETA: 6469 - ETA: 6471 - ETA: 6471 - ETA: 6473 - ETA: 6474 - ETA: 6475 - ETA: 6475 - ETA: 6477 - ETA: 6480 - ETA: 6479 - ETA: 6478 - ETA: 6480 - ETA: 6481 - ETA: 6482 - ETA: 6482 - ETA: 6483 - ETA: 6483 - ETA: 6484 - ETA: 6483 - ETA: 6484 - ETA: 6484 - ETA: 6484 - ETA: 6485 - ETA: 6485 - ETA: 6488 - ETA: 6487 - ETA: 6488 - ETA: 6489 - ETA: 6489 - ETA: 6490 - ETA: 6490 - ETA: 6490 - ETA: 6491 - ETA: 6493 - ETA: 6493 - ETA: 6494 - ETA: 6494 - ETA: 6495 - ETA: 6496 - ETA: 6498 - ETA: 6498 - ETA: 6498 - ETA: 6498 - ETA: 6501 - ETA: 6501 - ETA: 6501 - ETA: 6502 - ETA: 6502 - ETA: 6505 - ETA: 6507 - ETA: 6509 - ETA: 6512 - ETA: 6514 - ETA: 6515 - ETA: 6517 - ETA: 6518 - ETA: 6519 - ETA: 6518 - ETA: 6520 - ETA: 6522 - ETA: 6523 - ETA: 6527 - ETA: 6530 - ETA: 6532 - ETA: 6535 - ETA: 6541 - ETA: 6540 - ETA: 6548 - ETA: 6552 - ETA: 6557 - ETA: 6560 - ETA: 6563 - ETA: 6565 - ETA: 6567 - ETA: 6572 - ETA: 6575 - ETA: 6584 - ETA: 6592 - ETA: 6599 - ETA: 6605 - ETA: 6610 - ETA: 6613 - ETA: 6617 - ETA: 6619 - ETA: 6621 - ETA: 6626 - ETA: 6631 - ETA: 6633 - ETA: 6636 - ETA: 6638 - ETA: 6641 - ETA: 6642 - ETA: 6643 - ETA: 6645 - ETA: 6646 - ETA: 6647 - ETA: 6648 - ETA: 6649 - ETA: 6649 - ETA: 6649 - ETA: 6650 - ETA: 6653 - ETA: 6652 - ETA: 6652 - ETA: 6653 - ETA: 6653 - ETA: 6653 - ETA: 6656 - ETA: 6656 - ETA: 6656 - ETA: 6657 - ETA: 6659 - ETA: 6659 - ETA: 6660 - ETA: 6660 - ETA: 6660 - ETA: 6660 - ETA: 6661 - ETA: 6661 - ETA: 6661 - ETA: 6662 - ETA: 6662 - ETA: 6663 - ETA: 6663 - ETA: 6662 - ETA: 6665 - ETA: 6665 - ETA: 6665 - ETA: 6666 - ETA: 6666 - ETA: 6666 - ETA: 6666 - ETA: 6668 - ETA: 6668 - ETA: 6669 - ETA: 6669 - ETA: 6670 - ETA: 6669 - ETA: 6670 - ETA: 6668 - ETA: 6668 - ETA: 6669 - ETA: 6670 - ETA: 6669 - ETA: 6671 - ETA: 6672 - ETA: 6672 - ETA: 6672 - ETA: 6672 - ETA: 6673 - ETA: 6673 - ETA: 6673 - ETA: 6674 - ETA: 6674 - ETA: 6674 - ETA: 6674 - ETA: 6674 - ETA: 6674 - ETA: 6673 - ETA: 6673 - ETA: 6673 - ETA: 6674 - ETA: 6674 - ETA: 6674 - ETA: 6674 - ETA: 6674 - ETA: 6673 - ETA: 6673 - ETA: 6674 - ETA: 6674 - ETA: 6673 - ETA: 6672 - ETA: 6672 - ETA: 6673 - ETA: 6672 - ETA: 6673 - ETA: 6672 - ETA: 6671 - ETA: 6670 - ETA: 6670 - ETA: 6670 - ETA: 6670 - ETA: 6669 - ETA: 6669 - ETA: 6670 - ETA: 6670 - ETA: 6669 - ETA: 6668 - ETA: 6668 - ETA: 6667 - ETA: 6667 - ETA: 6669 - ETA: 6668 - ETA: 6666 - ETA: 6666 - ETA: 6666 - ETA: 6668 - ETA: 6667 - ETA: 6667 - ETA: 6668 - ETA: 6667 - ETA: 6667 - ETA: 6668 - ETA: 6668 - ETA: 6671 - ETA: 6670 - ETA: 6671 - ETA: 6672 - ETA: 6671 - ETA: 6672 - ETA: 6672 - ETA: 6673 - ETA: 6673 - ETA: 6673 - ETA: 6674 - ETA: 6676 - ETA: 6675 - ETA: 6676 - ETA: 6676 - ETA: 6676 - ETA: 6677 - ETA: 6677 - ETA: 6678 - ETA: 6679 - ETA: 6678 - ETA: 6679 - ETA: 6679 - ETA: 6679 - ETA: 6680 - ETA: 6679 - ETA: 6681 - ETA: 6681 - ETA: 6681 - ETA: 6683 - ETA: 6682 - ETA: 6684 - ETA: 6686 - ETA: 6689 - ETA: 6695 - ETA: 6703 - ETA: 6702 - ETA: 6707 - ETA: 6711 - ETA: 6715 - ETA: 6717 - ETA: 6719 - ETA: 6720 - ETA: 6723 - ETA: 6725 - ETA: 6724 - ETA: 6726 - ETA: 6728 - ETA: 6729 - ETA: 6729 - ETA: 6732 - ETA: 6732 - ETA: 6732 - ETA: 6732 - ETA: 6732 - ETA: 6736 - ETA: 6733 - ETA: 6734 - ETA: 6732 - ETA: 6732 - ETA: 6734 - ETA: 6734 - ETA: 6735 - ETA: 6735 - ETA: 6735 - ETA: 6735 - ETA: 6734 - ETA: 6734 - ETA: 6734 - ETA: 6734 - ETA: 6734 - ETA: 6734 - ETA: 6735 - ETA: 6737 - ETA: 6734 - ETA: 6735 - ETA: 6735 - ETA: 6739 - ETA: 6738 - ETA: 6738 - ETA: 6738 - ETA: 6740 - ETA: 6740 - ETA: 6741 - ETA: 6741 - ETA: 6741 - ETA: 6741 - ETA: 6742 - ETA: 6742 - ETA: 6741 - ETA: 6742 - ETA: 6742 - ETA: 6742 - ETA: 6743 - ETA: 6743 - ETA: 6743 - ETA: 6745 - ETA: 6745 - ETA: 6745 - ETA: 6746 - ETA: 6746 - ETA: 6746 - ETA: 6747 - ETA: 6747 - ETA: 6747 - ETA: 6747 - ETA: 6747 - ETA: 6747 - ETA: 6748 - ETA: 6748 - ETA: 6748 - ETA: 6749 - ETA: 6752 - ETA: 6751 - ETA: 6755 - ETA: 6757 - ETA: 6759 - ETA: 6761 - ETA: 6762 - ETA: 6764 - ETA: 6765 - ETA: 6766 - ETA: 6767 - ETA: 6767 - ETA: 6767 - ETA: 6769 - ETA: 6770 - ETA: 6772 - ETA: 6772 - ETA: 6776 - ETA: 6778 - ETA: 6780 - ETA: 6786 - ETA: 6793 - ETA: 6798 - ETA: 6805 - ETA: 6811 - ETA: 6815 - ETA: 6819 - ETA: 6822 - ETA: 6825 - ETA: 6827 - ETA: 6829 - ETA: 6830 - ETA: 6832 - ETA: 6832 - ETA: 6832 - ETA: 6834 - ETA: 6834 - ETA: 6835 - ETA: 6836 - ETA: 6837 - ETA: 6839 - ETA: 6841 - ETA: 6843 - ETA: 6844 - ETA: 6845 - ETA: 6847 - ETA: 6848 - ETA: 6849 - ETA: 6849 - ETA: 6850 - ETA: 6852 - ETA: 6851 - ETA: 6851 - ETA: 6852 - ETA: 6852 - ETA: 6854 - ETA: 6853 - ETA: 6855 - ETA: 6855 - ETA: 6854 - ETA: 6855 - ETA: 6855 - ETA: 6856 - ETA: 6856 - ETA: 6856 - ETA: 6857 - ETA: 6858 - ETA: 6858 - ETA: 6857 - ETA: 6856 - ETA: 6857 - ETA: 6857 - ETA: 6857 - ETA: 6857 - ETA: 6858 - ETA: 6858 - ETA: 6857 - ETA: 6858 - ETA: 6858 - ETA: 6858 - ETA: 6858 - ETA: 6857 - ETA: 6857 - ETA: 6857 - ETA: 6857 - ETA: 6856 - ETA: 6855 - ETA: 6855 - ETA: 6853 - ETA: 6853 - ETA: 6852 - ETA: 6850 - ETA: 6848 - ETA: 6847 - ETA: 6845 - ETA: 6842 - ETA: 6839 - ETA: 6837 - ETA: 6834 - ETA: 6832 - ETA: 6829 - ETA: 6825 - ETA: 6822 - ETA: 6817 - ETA: 6813 - ETA: 6809 - ETA: 6804 - ETA: 6800 - ETA: 6794 - ETA: 6790 - ETA: 6782 - ETA: 6777 - ETA: 6769 - ETA: 6768 - ETA: 6750 - ETA: 6742 - ETA: 6739 - ETA: 6731 - ETA: 6727 - ETA: 6720 - ETA: 6715 - ETA: 6714 - ETA: 6696 - ETA: 6691 - ETA: 6687 - ETA: 6683 - ETA: 6677 - ETA: 6673 - ETA: 6667 - ETA: 6663 - ETA: 6657 - ETA: 6651 - ETA: 6647 - ETA: 6642 - ETA: 6641 - ETA: 6635 - ETA: 6631 - ETA: 6625 - ETA: 6621 - ETA: 6615 - ETA: 6611 - ETA: 6605 - ETA: 6600 - ETA: 6595 - ETA: 6590 - ETA: 6585 - ETA: 6580 - ETA: 6575 - ETA: 6570 - ETA: 6565 - ETA: 6561 - ETA: 6550 - ETA: 6547 - ETA: 6543 - ETA: 6538 - ETA: 6539 - ETA: 6535 - ETA: 6526 - ETA: 6522 - ETA: 6519 - ETA: 6515 - ETA: 6513 - ETA: 6509 - ETA: 6507 - ETA: 6504 - ETA: 6501 - ETA: 6498 - ETA: 6494 - ETA: 6492 - ETA: 6488 - ETA: 6486 - ETA: 6482 - ETA: 6478 - ETA: 6475 - ETA: 6472 - ETA: 6469 - ETA: 6465 - ETA: 6463 - ETA: 6460 - ETA: 6459 - ETA: 6456 - ETA: 6453 - ETA: 6453 - ETA: 6451 - ETA: 6447 - ETA: 6444 - ETA: 6441 - ETA: 6437 - ETA: 6434 - ETA: 6432 - ETA: 6428 - ETA: 6425 - ETA: 6422 - ETA: 6417 - ETA: 6416 - ETA: 6412 - ETA: 6412 - ETA: 6405 - ETA: 6400 - ETA: 6400 - ETA: 6393 - ETA: 6392 - ETA: 6390 - ETA: 6389 - ETA: 6389 - ETA: 6387 - ETA: 6386 - ETA: 6384 - ETA: 6383 - ETA: 6382 - ETA: 6381 - ETA: 6380 - ETA: 6379 - ETA: 6379 - ETA: 6377 - ETA: 6377 - ETA: 6375 - ETA: 6374 - ETA: 6373 - ETA: 6372 - ETA: 6372 - ETA: 6372 - ETA: 6367 - ETA: 6366 - ETA: 6366 - ETA: 6367 - ETA: 6363 - ETA: 6364 - ETA: 6362 - ETA: 6361 - ETA: 6361 - ETA: 6360 - ETA: 6359 - ETA: 6361 - ETA: 6359 - ETA: 6360 - ETA: 6359 - ETA: 6359 - ETA: 6358 - ETA: 6358 - ETA: 6359 - ETA: 6358 - ETA: 6358 - ETA: 6358 - ETA: 6358 - ETA: 6358 - ETA: 6357 - ETA: 6357 - ETA: 6357 - ETA: 6356 - ETA: 6356 - ETA: 6356 - ETA: 6355 - ETA: 6356 - ETA: 6355 - ETA: 6356 - ETA: 6354 - ETA: 6356 - ETA: 6355 - ETA: 6355 - ETA: 6355 - ETA: 6355 - ETA: 6355 - ETA: 6356 - ETA: 6357 - ETA: 6359 - ETA: 6359 - ETA: 6361 - ETA: 6364 - ETA: 6365 - ETA: 6365 - ETA: 6367 - ETA: 6368 - ETA: 6369s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 87580672/553467096 [===>..........................] - ETA: 6370 - ETA: 6371 - ETA: 6372 - ETA: 6372 - ETA: 6372 - ETA: 6372 - ETA: 6373 - ETA: 6373 - ETA: 6373 - ETA: 6374 - ETA: 6374 - ETA: 6373 - ETA: 6373 - ETA: 6372 - ETA: 6371 - ETA: 6370 - ETA: 6368 - ETA: 6366 - ETA: 6365 - ETA: 6364 - ETA: 6362 - ETA: 6358 - ETA: 6358 - ETA: 6355 - ETA: 6346 - ETA: 6343 - ETA: 6340 - ETA: 6338 - ETA: 6337 - ETA: 6332 - ETA: 6331 - ETA: 6327 - ETA: 6327 - ETA: 6318 - ETA: 6317 - ETA: 6313 - ETA: 6312 - ETA: 6309 - ETA: 6308 - ETA: 6309 - ETA: 6302 - ETA: 6301 - ETA: 6298 - ETA: 6297 - ETA: 6295 - ETA: 6293 - ETA: 6289 - ETA: 6288 - ETA: 6286 - ETA: 6287 - ETA: 6281 - ETA: 6279 - ETA: 6278 - ETA: 6277 - ETA: 6276 - ETA: 6274 - ETA: 6273 - ETA: 6272 - ETA: 6270 - ETA: 6268 - ETA: 6267 - ETA: 6268 - ETA: 6264 - ETA: 6262 - ETA: 6262 - ETA: 6260 - ETA: 6259 - ETA: 6260 - ETA: 6257 - ETA: 6257 - ETA: 6256 - ETA: 6256 - ETA: 6256 - ETA: 6256 - ETA: 6255 - ETA: 6255 - ETA: 6254 - ETA: 6254 - ETA: 6255 - ETA: 6255 - ETA: 6254 - ETA: 6253 - ETA: 6253 - ETA: 6253 - ETA: 6254 - ETA: 6253 - ETA: 6253 - ETA: 6252 - ETA: 6251 - ETA: 6251 - ETA: 6251 - ETA: 6250 - ETA: 6251 - ETA: 6251 - ETA: 6249 - ETA: 6249 - ETA: 6248 - ETA: 6248 - ETA: 6247 - ETA: 6246 - ETA: 6245 - ETA: 6247 - ETA: 6245 - ETA: 6245s"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\xiaogao\\\\.keras\\\\models\\\\vgg16_weights_tf_dim_ordering_tf_kernels.h5'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\xiaogao\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\keras\\utils\\data_utils.py\u001b[0m in \u001b[0;36mget_file\u001b[1;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir)\u001b[0m\n\u001b[0;32m    206\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 207\u001b[1;33m                 \u001b[0murlretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morigin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdl_progress\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    208\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mURLError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\xiaogao\\AppData\\Local\\Continuum\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m    276\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 277\u001b[1;33m                 \u001b[0mblock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    278\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\xiaogao\\AppData\\Local\\Continuum\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    448\u001b[0m             \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m             \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\xiaogao\\AppData\\Local\\Continuum\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m         \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\xiaogao\\AppData\\Local\\Continuum\\Anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    585\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\xiaogao\\AppData\\Local\\Continuum\\Anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1001\u001b[0m                   self.__class__)\n\u001b[1;32m-> 1002\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1003\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\xiaogao\\AppData\\Local\\Continuum\\Anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m    864\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 865\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    866\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\xiaogao\\AppData\\Local\\Continuum\\Anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m    624\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m             \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-b499363526b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVGG16\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'imagenet'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minclude_top\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# set figure size\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\xiaogao\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\keras\\applications\\vgg16.py\u001b[0m in \u001b[0;36mVGG16\u001b[1;34m(include_top, weights, input_tensor, input_shape, pooling, classes)\u001b[0m\n\u001b[0;32m    162\u001b[0m             weights_path = get_file('vgg16_weights_tf_dim_ordering_tf_kernels.h5',\n\u001b[0;32m    163\u001b[0m                                     \u001b[0mWEIGHTS_PATH\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m                                     cache_subdir='models')\n\u001b[0m\u001b[0;32m    165\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m             weights_path = get_file('vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
      "\u001b[1;32mC:\\Users\\xiaogao\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\keras\\utils\\data_utils.py\u001b[0m in \u001b[0;36mget_file\u001b[1;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir)\u001b[0m\n\u001b[0;32m    212\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mException\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 214\u001b[1;33m                 \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    215\u001b[0m             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m         \u001b[0mProgressTracker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\xiaogao\\\\.keras\\\\models\\\\vgg16_weights_tf_dim_ordering_tf_kernels.h5'"
     ]
    }
   ],
   "source": [
    "model1 = VGG16(weights='imagenet', include_top=True)\n",
    "\n",
    "def inference(imgs, preprocess=True):\n",
    "    plt.figure(figsize = (15, 7)) # set figure size\n",
    "    for i in range(len(imgs)):\n",
    "        plt.subplot(1, len(imgs), i+1)\n",
    "        img = cv2.imread(imgs[i])\n",
    "        img = img[:, :, ::-1]\n",
    "        x = cv2.resize(img, dsize=(224, 224))\n",
    "        x = np.expand_dims(x, axis=0) # expand to 3 channels\n",
    "        if preprocess:\n",
    "            x = preprocess_input(x)\n",
    "        y_pred = model1.predict(x)\n",
    "        _, label, prob = decode_predictions(y_pred, top=1)[0][0] # only pick top 1 prediction\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.title('%s: %.2f %%' % (label, prob * 100), size = 15)\n",
    "    plt.show()\n",
    "    \n",
    "inference(['./train/cat1.jpg', './train/cat2.jpg', './train/cat3.jpg', './train/cat4.jpg'], False)\n",
    "inference(['./train/cat1.jpg', './train/cat2.jpg', './train/cat3.jpg', './train/cat4.jpg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "\n",
    "- Although the model hasn't seen the images in the training set, it does have seen the image type \"cat\" from imagenet. So the model can provide fairly accurate prediction results.\n",
    "- The result of preprocessing may not be always helpful. The probability of 1st image is down from 65% to 15% while that of 3rd image is up from 71% to 99%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Apply pre-trained model on training data but with new classifer\n",
    "\n",
    "**Two solutions**\n",
    "- Solution 1: Load the entire pre-trained model plus weights, replace with new classifier.\n",
    "- Solution 2: Create the model with the same structure of the pre-trained model and only load the pre-trained weights, replace with new classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Data Preparation\n",
    "\n",
    "**Steps**\n",
    "- Read input：cv2.imread\n",
    "- Change image size：cv2.resize\n",
    "- Define image type：cat = 0, dog = 1\n",
    "- Shuffle the sequence of images and classification：shuffle\n",
    "- Split training set into training and validation sets：train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12500/12500 [02:09<00:00, 96.27it/s] \n",
      "100%|██████████| 12500/12500 [01:54<00:00, 109.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Size = 3.00 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "shape = 224 # VGG16 input size = 224*224\n",
    "label = np.array([0] * 12500 + [1] * 12500)\n",
    "data = np.zeros((25000, shape, shape, 3), dtype=np.uint8)\n",
    "\n",
    "for i in tqdm(range(12500)):\n",
    "    img = cv2.imread('./train/cat.%s.jpg' % str(i))\n",
    "    img = img[:, :, ::-1]\n",
    "    data[i] = cv2.resize(img, (shape, shape))\n",
    "    \n",
    "for i in tqdm(range(12500)):\n",
    "    img = cv2.imread('./train/dog.%s.jpg' % str(i))\n",
    "    img = img[:, :, ::-1]\n",
    "    data[i + 12500] = cv2.resize(img, (shape, shape))\n",
    "    \n",
    "print('Training Data Size = %.2f GB' % (sys.getsizeof(data)/1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(data, label, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12500/12500 [01:30<00:00, 138.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Data Size = 1.00 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test = np.zeros((12500, shape, shape, 3), dtype=np.uint8)\n",
    "for i in tqdm(range(12500)):\n",
    "    img = cv2.imread('./test/%s.jpg' % str(i + 1))\n",
    "    img = img[:, :, ::-1]\n",
    "    test[i] = cv2.resize(img, (shape, shape))\n",
    "print('Testing Data Size = %.2f GB' % (sys.getsizeof(test)/1024**3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load the entire pre-trained model plus weights, replace with new classifier.\n",
    "\n",
    "**Steps**\n",
    "- Load pre-trained model and weight, excluding the original classifier\n",
    "- Lock the layers of the pre-trained model to prevent the change during the training process: layers.trainable = False\n",
    "- Add the new classifier to the end of the model. Choose sigmoid or softmax based on the number of types that need to classify\n",
    "- Compire the model.Choose binary_crossentropy or categorical_crossentropy based on the number of types that need to classify\n",
    "- Check the number of trainable weights: create function get_param_count() to count unlocked parameters\n",
    "- Train the model\n",
    "  - Use small batch size so that we can get high precision even with a few epochs\n",
    "  - No need to use too many epochs, 5 ~ 10 is enough. Because the number of trainable parameters is small (~500), it can achieve the optimal without too many epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First I use VGG16 (22 layers) model for transfer learning.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def get_params_count(model):\n",
    "    trainable = int(np.sum([K.count_params(p) for p in set(model.trainable_weights)]))\n",
    "    non_trainable = int(np.sum([K.count_params(p) for p in set(model.non_trainable_weights)]))\n",
    "    return trainable, non_trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 14,715,201\n",
      "Trainable params: 513\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "Model has 22 layers.\n"
     ]
    }
   ],
   "source": [
    "base_model = VGG16(input_shape=(224, 224, 3), weights='imagenet', include_top=False, pooling='avg')\n",
    "\n",
    "for layers in base_model.layers:\n",
    "    layers.trainable = False\n",
    "\n",
    "y = Dropout(0.5)(base_model.output)\n",
    "y = Dense(1, activation='sigmoid')(y)\n",
    "\n",
    "model1 = Model(inputs=base_model.input, outputs=y)\n",
    "model1.compile(loss='binary_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "model1.summary()\n",
    "print('Model has %d layers.' % len(model1.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 509s - loss: 1.1711 - acc: 0.8045 - val_loss: 0.1745 - val_acc: 0.9540\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 484s - loss: 0.3822 - acc: 0.9157 - val_loss: 0.1281 - val_acc: 0.9636\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 483s - loss: 0.2848 - acc: 0.9304 - val_loss: 0.1187 - val_acc: 0.9658\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 484s - loss: 0.2421 - acc: 0.9394 - val_loss: 0.1072 - val_acc: 0.9662\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 484s - loss: 0.2120 - acc: 0.9415 - val_loss: 0.0973 - val_acc: 0.9680\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6a84f0ce50>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(x=X_train, y=y_train, batch_size=16, epochs=5, validation_data=(X_val, y_val), shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now let's try ResNet50 model (178 layers). **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_2 (InputLayer)             (None, 224, 224, 3)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D) (None, 230, 230, 3)   0           input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                   (None, 112, 112, 64)  9472        zero_padding2d_1[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)    (None, 112, 112, 64)  256         conv1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 112, 112, 64)  0           bn_conv1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)   (None, 55, 55, 64)    0           activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)          (None, 55, 55, 64)    4160        max_pooling2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNormalizatio (None, 55, 55, 64)    256         res2a_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 55, 55, 64)    0           bn2a_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)          (None, 55, 55, 64)    36928       activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNormalizatio (None, 55, 55, 64)    256         res2a_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 55, 55, 64)    0           bn2a_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res2a_branch2c (Conv2D)          (None, 55, 55, 256)   16640       activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)           (None, 55, 55, 256)   16640       max_pooling2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "bn2a_branch2c (BatchNormalizatio (None, 55, 55, 256)   1024        res2a_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNormalization (None, 55, 55, 256)   1024        res2a_branch1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "add_1 (Add)                      (None, 55, 55, 256)   0           bn2a_branch2c[0][0]              \n",
      "                                                                   bn2a_branch1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 55, 55, 256)   0           add_1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)          (None, 55, 55, 64)    16448       activation_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNormalizatio (None, 55, 55, 64)    256         res2b_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_5 (Activation)        (None, 55, 55, 64)    0           bn2b_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)          (None, 55, 55, 64)    36928       activation_5[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNormalizatio (None, 55, 55, 64)    256         res2b_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_6 (Activation)        (None, 55, 55, 64)    0           bn2b_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res2b_branch2c (Conv2D)          (None, 55, 55, 256)   16640       activation_6[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "bn2b_branch2c (BatchNormalizatio (None, 55, 55, 256)   1024        res2b_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_2 (Add)                      (None, 55, 55, 256)   0           bn2b_branch2c[0][0]              \n",
      "                                                                   activation_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_7 (Activation)        (None, 55, 55, 256)   0           add_2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "res2c_branch2a (Conv2D)          (None, 55, 55, 64)    16448       activation_7[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "bn2c_branch2a (BatchNormalizatio (None, 55, 55, 64)    256         res2c_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_8 (Activation)        (None, 55, 55, 64)    0           bn2c_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res2c_branch2b (Conv2D)          (None, 55, 55, 64)    36928       activation_8[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "bn2c_branch2b (BatchNormalizatio (None, 55, 55, 64)    256         res2c_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_9 (Activation)        (None, 55, 55, 64)    0           bn2c_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res2c_branch2c (Conv2D)          (None, 55, 55, 256)   16640       activation_9[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "bn2c_branch2c (BatchNormalizatio (None, 55, 55, 256)   1024        res2c_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_3 (Add)                      (None, 55, 55, 256)   0           bn2c_branch2c[0][0]              \n",
      "                                                                   activation_7[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_10 (Activation)       (None, 55, 55, 256)   0           add_3[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "res3a_branch2a (Conv2D)          (None, 28, 28, 128)   32896       activation_10[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3a_branch2a (BatchNormalizatio (None, 28, 28, 128)   512         res3a_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_11 (Activation)       (None, 28, 28, 128)   0           bn3a_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3a_branch2b (Conv2D)          (None, 28, 28, 128)   147584      activation_11[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3a_branch2b (BatchNormalizatio (None, 28, 28, 128)   512         res3a_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_12 (Activation)       (None, 28, 28, 128)   0           bn3a_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3a_branch2c (Conv2D)          (None, 28, 28, 512)   66048       activation_12[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3a_branch1 (Conv2D)           (None, 28, 28, 512)   131584      activation_10[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3a_branch2c (BatchNormalizatio (None, 28, 28, 512)   2048        res3a_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn3a_branch1 (BatchNormalization (None, 28, 28, 512)   2048        res3a_branch1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "add_4 (Add)                      (None, 28, 28, 512)   0           bn3a_branch2c[0][0]              \n",
      "                                                                   bn3a_branch1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_13 (Activation)       (None, 28, 28, 512)   0           add_4[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "res3b_branch2a (Conv2D)          (None, 28, 28, 128)   65664       activation_13[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3b_branch2a (BatchNormalizatio (None, 28, 28, 128)   512         res3b_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_14 (Activation)       (None, 28, 28, 128)   0           bn3b_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3b_branch2b (Conv2D)          (None, 28, 28, 128)   147584      activation_14[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3b_branch2b (BatchNormalizatio (None, 28, 28, 128)   512         res3b_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_15 (Activation)       (None, 28, 28, 128)   0           bn3b_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3b_branch2c (Conv2D)          (None, 28, 28, 512)   66048       activation_15[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3b_branch2c (BatchNormalizatio (None, 28, 28, 512)   2048        res3b_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_5 (Add)                      (None, 28, 28, 512)   0           bn3b_branch2c[0][0]              \n",
      "                                                                   activation_13[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_16 (Activation)       (None, 28, 28, 512)   0           add_5[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "res3c_branch2a (Conv2D)          (None, 28, 28, 128)   65664       activation_16[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3c_branch2a (BatchNormalizatio (None, 28, 28, 128)   512         res3c_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_17 (Activation)       (None, 28, 28, 128)   0           bn3c_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3c_branch2b (Conv2D)          (None, 28, 28, 128)   147584      activation_17[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3c_branch2b (BatchNormalizatio (None, 28, 28, 128)   512         res3c_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_18 (Activation)       (None, 28, 28, 128)   0           bn3c_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3c_branch2c (Conv2D)          (None, 28, 28, 512)   66048       activation_18[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3c_branch2c (BatchNormalizatio (None, 28, 28, 512)   2048        res3c_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_6 (Add)                      (None, 28, 28, 512)   0           bn3c_branch2c[0][0]              \n",
      "                                                                   activation_16[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_19 (Activation)       (None, 28, 28, 512)   0           add_6[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "res3d_branch2a (Conv2D)          (None, 28, 28, 128)   65664       activation_19[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3d_branch2a (BatchNormalizatio (None, 28, 28, 128)   512         res3d_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_20 (Activation)       (None, 28, 28, 128)   0           bn3d_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3d_branch2b (Conv2D)          (None, 28, 28, 128)   147584      activation_20[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3d_branch2b (BatchNormalizatio (None, 28, 28, 128)   512         res3d_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_21 (Activation)       (None, 28, 28, 128)   0           bn3d_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3d_branch2c (Conv2D)          (None, 28, 28, 512)   66048       activation_21[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3d_branch2c (BatchNormalizatio (None, 28, 28, 512)   2048        res3d_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_7 (Add)                      (None, 28, 28, 512)   0           bn3d_branch2c[0][0]              \n",
      "                                                                   activation_19[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_22 (Activation)       (None, 28, 28, 512)   0           add_7[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "res4a_branch2a (Conv2D)          (None, 14, 14, 256)   131328      activation_22[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4a_branch2a (BatchNormalizatio (None, 14, 14, 256)   1024        res4a_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_23 (Activation)       (None, 14, 14, 256)   0           bn4a_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4a_branch2b (Conv2D)          (None, 14, 14, 256)   590080      activation_23[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4a_branch2b (BatchNormalizatio (None, 14, 14, 256)   1024        res4a_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_24 (Activation)       (None, 14, 14, 256)   0           bn4a_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4a_branch2c (Conv2D)          (None, 14, 14, 1024)  263168      activation_24[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4a_branch1 (Conv2D)           (None, 14, 14, 1024)  525312      activation_22[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4a_branch2c (BatchNormalizatio (None, 14, 14, 1024)  4096        res4a_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn4a_branch1 (BatchNormalization (None, 14, 14, 1024)  4096        res4a_branch1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "add_8 (Add)                      (None, 14, 14, 1024)  0           bn4a_branch2c[0][0]              \n",
      "                                                                   bn4a_branch1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_25 (Activation)       (None, 14, 14, 1024)  0           add_8[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "res4b_branch2a (Conv2D)          (None, 14, 14, 256)   262400      activation_25[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4b_branch2a (BatchNormalizatio (None, 14, 14, 256)   1024        res4b_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_26 (Activation)       (None, 14, 14, 256)   0           bn4b_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4b_branch2b (Conv2D)          (None, 14, 14, 256)   590080      activation_26[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4b_branch2b (BatchNormalizatio (None, 14, 14, 256)   1024        res4b_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_27 (Activation)       (None, 14, 14, 256)   0           bn4b_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4b_branch2c (Conv2D)          (None, 14, 14, 1024)  263168      activation_27[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4b_branch2c (BatchNormalizatio (None, 14, 14, 1024)  4096        res4b_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_9 (Add)                      (None, 14, 14, 1024)  0           bn4b_branch2c[0][0]              \n",
      "                                                                   activation_25[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_28 (Activation)       (None, 14, 14, 1024)  0           add_9[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "res4c_branch2a (Conv2D)          (None, 14, 14, 256)   262400      activation_28[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4c_branch2a (BatchNormalizatio (None, 14, 14, 256)   1024        res4c_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_29 (Activation)       (None, 14, 14, 256)   0           bn4c_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4c_branch2b (Conv2D)          (None, 14, 14, 256)   590080      activation_29[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4c_branch2b (BatchNormalizatio (None, 14, 14, 256)   1024        res4c_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_30 (Activation)       (None, 14, 14, 256)   0           bn4c_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4c_branch2c (Conv2D)          (None, 14, 14, 1024)  263168      activation_30[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4c_branch2c (BatchNormalizatio (None, 14, 14, 1024)  4096        res4c_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_10 (Add)                     (None, 14, 14, 1024)  0           bn4c_branch2c[0][0]              \n",
      "                                                                   activation_28[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_31 (Activation)       (None, 14, 14, 1024)  0           add_10[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res4d_branch2a (Conv2D)          (None, 14, 14, 256)   262400      activation_31[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4d_branch2a (BatchNormalizatio (None, 14, 14, 256)   1024        res4d_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_32 (Activation)       (None, 14, 14, 256)   0           bn4d_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4d_branch2b (Conv2D)          (None, 14, 14, 256)   590080      activation_32[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4d_branch2b (BatchNormalizatio (None, 14, 14, 256)   1024        res4d_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_33 (Activation)       (None, 14, 14, 256)   0           bn4d_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4d_branch2c (Conv2D)          (None, 14, 14, 1024)  263168      activation_33[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4d_branch2c (BatchNormalizatio (None, 14, 14, 1024)  4096        res4d_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_11 (Add)                     (None, 14, 14, 1024)  0           bn4d_branch2c[0][0]              \n",
      "                                                                   activation_31[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_34 (Activation)       (None, 14, 14, 1024)  0           add_11[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res4e_branch2a (Conv2D)          (None, 14, 14, 256)   262400      activation_34[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4e_branch2a (BatchNormalizatio (None, 14, 14, 256)   1024        res4e_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_35 (Activation)       (None, 14, 14, 256)   0           bn4e_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4e_branch2b (Conv2D)          (None, 14, 14, 256)   590080      activation_35[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4e_branch2b (BatchNormalizatio (None, 14, 14, 256)   1024        res4e_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_36 (Activation)       (None, 14, 14, 256)   0           bn4e_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4e_branch2c (Conv2D)          (None, 14, 14, 1024)  263168      activation_36[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4e_branch2c (BatchNormalizatio (None, 14, 14, 1024)  4096        res4e_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_12 (Add)                     (None, 14, 14, 1024)  0           bn4e_branch2c[0][0]              \n",
      "                                                                   activation_34[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_37 (Activation)       (None, 14, 14, 1024)  0           add_12[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res4f_branch2a (Conv2D)          (None, 14, 14, 256)   262400      activation_37[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4f_branch2a (BatchNormalizatio (None, 14, 14, 256)   1024        res4f_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_38 (Activation)       (None, 14, 14, 256)   0           bn4f_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4f_branch2b (Conv2D)          (None, 14, 14, 256)   590080      activation_38[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4f_branch2b (BatchNormalizatio (None, 14, 14, 256)   1024        res4f_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_39 (Activation)       (None, 14, 14, 256)   0           bn4f_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4f_branch2c (Conv2D)          (None, 14, 14, 1024)  263168      activation_39[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4f_branch2c (BatchNormalizatio (None, 14, 14, 1024)  4096        res4f_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_13 (Add)                     (None, 14, 14, 1024)  0           bn4f_branch2c[0][0]              \n",
      "                                                                   activation_37[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_40 (Activation)       (None, 14, 14, 1024)  0           add_13[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res5a_branch2a (Conv2D)          (None, 7, 7, 512)     524800      activation_40[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn5a_branch2a (BatchNormalizatio (None, 7, 7, 512)     2048        res5a_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_41 (Activation)       (None, 7, 7, 512)     0           bn5a_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res5a_branch2b (Conv2D)          (None, 7, 7, 512)     2359808     activation_41[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn5a_branch2b (BatchNormalizatio (None, 7, 7, 512)     2048        res5a_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_42 (Activation)       (None, 7, 7, 512)     0           bn5a_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res5a_branch2c (Conv2D)          (None, 7, 7, 2048)    1050624     activation_42[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res5a_branch1 (Conv2D)           (None, 7, 7, 2048)    2099200     activation_40[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn5a_branch2c (BatchNormalizatio (None, 7, 7, 2048)    8192        res5a_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn5a_branch1 (BatchNormalization (None, 7, 7, 2048)    8192        res5a_branch1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "add_14 (Add)                     (None, 7, 7, 2048)    0           bn5a_branch2c[0][0]              \n",
      "                                                                   bn5a_branch1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_43 (Activation)       (None, 7, 7, 2048)    0           add_14[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res5b_branch2a (Conv2D)          (None, 7, 7, 512)     1049088     activation_43[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn5b_branch2a (BatchNormalizatio (None, 7, 7, 512)     2048        res5b_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_44 (Activation)       (None, 7, 7, 512)     0           bn5b_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res5b_branch2b (Conv2D)          (None, 7, 7, 512)     2359808     activation_44[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn5b_branch2b (BatchNormalizatio (None, 7, 7, 512)     2048        res5b_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_45 (Activation)       (None, 7, 7, 512)     0           bn5b_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res5b_branch2c (Conv2D)          (None, 7, 7, 2048)    1050624     activation_45[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn5b_branch2c (BatchNormalizatio (None, 7, 7, 2048)    8192        res5b_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_15 (Add)                     (None, 7, 7, 2048)    0           bn5b_branch2c[0][0]              \n",
      "                                                                   activation_43[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_46 (Activation)       (None, 7, 7, 2048)    0           add_15[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res5c_branch2a (Conv2D)          (None, 7, 7, 512)     1049088     activation_46[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn5c_branch2a (BatchNormalizatio (None, 7, 7, 512)     2048        res5c_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_47 (Activation)       (None, 7, 7, 512)     0           bn5c_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res5c_branch2b (Conv2D)          (None, 7, 7, 512)     2359808     activation_47[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn5c_branch2b (BatchNormalizatio (None, 7, 7, 512)     2048        res5c_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_48 (Activation)       (None, 7, 7, 512)     0           bn5c_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res5c_branch2c (Conv2D)          (None, 7, 7, 2048)    1050624     activation_48[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn5c_branch2c (BatchNormalizatio (None, 7, 7, 2048)    8192        res5c_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_16 (Add)                     (None, 7, 7, 2048)    0           bn5c_branch2c[0][0]              \n",
      "                                                                   activation_46[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_49 (Activation)       (None, 7, 7, 2048)    0           add_16[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "avg_pool (AveragePooling2D)      (None, 1, 1, 2048)    0           activation_49[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_2 (Glob (None, 2048)          0           avg_pool[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 2048)          0           global_average_pooling2d_2[0][0] \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 1)             2049        dropout_2[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 23,589,761\n",
      "Trainable params: 2,049\n",
      "Non-trainable params: 23,587,712\n",
      "____________________________________________________________________________________________________\n",
      "Model has 178 layers.\n"
     ]
    }
   ],
   "source": [
    "base_model = ResNet50(input_shape=(224, 224, 3), weights='imagenet', include_top=False, pooling='avg')\n",
    "\n",
    "for layers in base_model.layers:\n",
    "    layers.trainable = False\n",
    "\n",
    "y = Dropout(0.25)(base_model.output)\n",
    "y = Dense(1, activation='sigmoid')(y)\n",
    "\n",
    "model2 = Model(inputs=base_model.input, outputs=y)\n",
    "model2.compile(loss='binary_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "model2.summary()\n",
    "print('Model has %d layers.' % len(model2.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 297s - loss: 0.1764 - acc: 0.9321 - val_loss: 0.0751 - val_acc: 0.9748\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 296s - loss: 0.1040 - acc: 0.9605 - val_loss: 0.0639 - val_acc: 0.9766\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 296s - loss: 0.0979 - acc: 0.9624 - val_loss: 0.0603 - val_acc: 0.9770\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 296s - loss: 0.0926 - acc: 0.9644 - val_loss: 0.0574 - val_acc: 0.9776\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 296s - loss: 0.0868 - acc: 0.9657 - val_loss: 0.0566 - val_acc: 0.9772\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f69ca887e50>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(x=X_train, y=y_train, batch_size=16, epochs=5, validation_data=(X_val, y_val), shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Create the model with the same structure of the pre-trained model and only load the pre-trained weights, replace with new classifier\n",
    "\n",
    "- It's much complicated than solution 1 as I have to create the model with the same structure by hand.\n",
    "- It requires the model I create is the same as the pre-trained model (the size of all the weight matrixes should be the same). Otherwise, it will throw the error when loading the weights. The advantage of this solution is I can change the input size, hyper-paramters freely. For some dataset with small image size (e.g. CIRAR10), I have to either enlarge the image size or use solution 2 to modify the input size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Input(shape=(32, 32, 3))\n",
    "y = x\n",
    "y = Convolution2D(filters=64, kernel_size=3, strides=1, padding='same', activation='relu', kernel_initializer='he_normal')(y)\n",
    "y = Convolution2D(filters=64, kernel_size=3, strides=1, padding='same', activation='relu', kernel_initializer='he_normal')(y)\n",
    "y = MaxPooling2D(pool_size=2, strides=2, padding='same')(y)\n",
    "\n",
    "y = Convolution2D(filters=128, kernel_size=3, strides=1, padding='same', activation='relu', kernel_initializer='he_normal')(y)\n",
    "y = Convolution2D(filters=128, kernel_size=3, strides=1, padding='same', activation='relu', kernel_initializer='he_normal')(y)\n",
    "y = MaxPooling2D(pool_size=2, strides=2, padding='same')(y)\n",
    "\n",
    "y = Convolution2D(filters=256, kernel_size=3, strides=1, padding='same', activation='relu', kernel_initializer='he_normal')(y)\n",
    "y = Convolution2D(filters=256, kernel_size=3, strides=1, padding='same', activation='relu', kernel_initializer='he_normal')(y)\n",
    "y = Convolution2D(filters=256, kernel_size=3, strides=1, padding='same', activation='relu', kernel_initializer='he_normal')(y)\n",
    "y = MaxPooling2D(pool_size=2, strides=2, padding='same')(y)\n",
    "\n",
    "y = Convolution2D(filters=512, kernel_size=3, strides=1, padding='same', activation='relu', kernel_initializer='he_normal')(y)\n",
    "y = Convolution2D(filters=512, kernel_size=3, strides=1, padding='same', activation='relu', kernel_initializer='he_normal')(y)\n",
    "y = Convolution2D(filters=512, kernel_size=3, strides=1, padding='same', activation='relu', kernel_initializer='he_normal')(y)\n",
    "y = MaxPooling2D(pool_size=2, strides=2, padding='same')(y)\n",
    "\n",
    "y = Convolution2D(filters=512, kernel_size=3, strides=1, padding='same', activation='relu', kernel_initializer='he_normal')(y)\n",
    "y = Convolution2D(filters=512, kernel_size=3, strides=1, padding='same', activation='relu', kernel_initializer='he_normal')(y)\n",
    "y = Convolution2D(filters=512, kernel_size=3, strides=1, padding='same', activation='relu', kernel_initializer='he_normal')(y)\n",
    "y = MaxPooling2D(pool_size=2, strides=2, padding='same')(y)\n",
    "\n",
    "y = GlobalAveragePooling2D()(y)\n",
    "# y = Dropout(0.5)(y)\n",
    "# y = Dense(10, activation='softmax')(y)\n",
    "\n",
    "modelx = Model(inputs=x, outputs=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 4, 4, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_3 ( (None, 512)               0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelx.load_weights('/home/ubuntu/.keras/models/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
    "modelx.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.topology.InputLayer at 0x7f69c8100bd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f69cae6b1d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f69caeda450>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7f6a84e4cb50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f69cad3d290>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f69cac775d0>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7f69cab75990>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f69caa75ad0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f69c80a3f10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f69c80c2850>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7f69c80d21d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f69c8070550>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f69c8081c10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f69c8091510>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7f69c8036f90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f69b87ba950>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f69b87c8a90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f69b87dca90>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7f69b87ec490>,\n",
       " <keras.layers.pooling.GlobalAveragePooling2D at 0x7f69b878e790>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelx.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 3, 64)\n",
      "(64,)\n"
     ]
    }
   ],
   "source": [
    "[weights, bias] = modelx.layers[1].get_weights()\n",
    "print(weights.shape)\n",
    "print(bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.42947057,  0.55037946,  0.4800154 ],\n",
       "        [ 0.373467  ,  0.44007453,  0.4085474 ],\n",
       "        [-0.06136011, -0.08138704, -0.06514555]],\n",
       "\n",
       "       [[ 0.27476987,  0.34573907,  0.31047726],\n",
       "        [ 0.03868078,  0.04063221,  0.05020237],\n",
       "        [-0.36722335, -0.45350131, -0.40338343]],\n",
       "\n",
       "       [[-0.05746817, -0.05863491, -0.05087169],\n",
       "        [-0.26224968, -0.33066967, -0.28522751],\n",
       "        [-0.35009676, -0.4850302 , -0.41851634]]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[:, :, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73429835"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: Apply pre-trained model on training data with new classifer and fine tune the last several layers\n",
    "\n",
    "**Steps**\n",
    "- Here I use the model2 created above. \n",
    "- Unblock the last several layers and train them 选定模型中较靠后的部分解冻，参与训练\n",
    "- Since several layers form a combination (e.g. conv + batchNorm + acitvation), we'd better block/unblock the combination.\n",
    "- Unblock the layers from high (output) to low (input). Train 5 epochs when unblocking on part. Then check if the loss and accuracy have improved. If the answer is yes, then we can continue training. Otherwise, we can unblock more layers until the training result has improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.core.Activation at 0x7f69cb4f8bd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f69cb4f8d90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f69cb4c7e90>,\n",
       " <keras.layers.core.Activation at 0x7f69cb498fd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f69cb498590>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f69cb43c610>,\n",
       " <keras.layers.core.Activation at 0x7f69cb370f50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f69cb3b9f90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f69cb313ed0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f69cb3dd590>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f69cb35be50>,\n",
       " <keras.layers.merge.Add at 0x7f69cb2e6c90>,\n",
       " <keras.layers.core.Activation at 0x7f69cb28df50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f69cb27d950>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f69cb20ae50>,\n",
       " <keras.layers.core.Activation at 0x7f69cb1d7e50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f69cb220c10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f69cb1c3490>,\n",
       " <keras.layers.core.Activation at 0x7f69cb0fadd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f69cb144dd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f69cb165410>,\n",
       " <keras.layers.merge.Add at 0x7f69cb09dd50>,\n",
       " <keras.layers.core.Activation at 0x7f69cb0e2d50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f69cb0e2750>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f69cb034cd0>,\n",
       " <keras.layers.core.Activation at 0x7f69cafbff10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f69cb008f50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f69cafa8550>,\n",
       " <keras.layers.core.Activation at 0x7f69caf60e90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f69caf2ac50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f69caf4c4d0>,\n",
       " <keras.layers.merge.Add at 0x7f69cae80e10>,\n",
       " <keras.layers.core.Activation at 0x7f69caec9bd0>,\n",
       " <keras.layers.pooling.AveragePooling2D at 0x7f69caec9d90>,\n",
       " <keras.layers.pooling.GlobalAveragePooling2D at 0x7f69cae99d90>,\n",
       " <keras.layers.core.Dropout at 0x7f69cad8da50>,\n",
       " <keras.layers.core.Dense at 0x7f69cad8db10>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.layers[-37:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable = 14453249, Non-Trainable = 9136512\n"
     ]
    }
   ],
   "source": [
    "for layers in model2.layers[-35:]:\n",
    "    layers.trainable = True\n",
    "    \n",
    "print('Trainable = %d, Non-Trainable = %d' % (get_params_count(model2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 294s - loss: 0.0860 - acc: 0.9668 - val_loss: 0.0604 - val_acc: 0.9756\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 295s - loss: 0.0870 - acc: 0.9647 - val_loss: 0.0583 - val_acc: 0.9800\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 295s - loss: 0.0795 - acc: 0.9698 - val_loss: 0.0535 - val_acc: 0.9788\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 295s - loss: 0.0830 - acc: 0.9667 - val_loss: 0.0519 - val_acc: 0.9796\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 295s - loss: 0.0841 - acc: 0.9675 - val_loss: 0.0520 - val_acc: 0.9810\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 295s - loss: 0.0796 - acc: 0.9687 - val_loss: 0.0518 - val_acc: 0.9788\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 295s - loss: 0.0799 - acc: 0.9689 - val_loss: 0.0510 - val_acc: 0.9806\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 295s - loss: 0.0802 - acc: 0.9694 - val_loss: 0.0504 - val_acc: 0.9792\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 295s - loss: 0.0781 - acc: 0.9680 - val_loss: 0.0495 - val_acc: 0.9810\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 295s - loss: 0.0811 - acc: 0.9698 - val_loss: 0.0496 - val_acc: 0.9794\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6a874e7b50>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(x=X_train, y=y_train, batch_size=16, epochs=10, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2.save('ResNet_Finetune_last3_epoch5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
